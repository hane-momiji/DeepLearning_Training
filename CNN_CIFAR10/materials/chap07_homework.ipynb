{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "\n",
    "今Lessonで学んだことに工夫を加えて、CNNでより高精度なCIFAR10の分類器を実装してみましょう。精度上位者はリーダーボードに載ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目標値\n",
    "\n",
    "Accuracy 78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ルール\n",
    "\n",
    "- 訓練データはx_train、 t_train、テストデータはx_testで与えられます。\n",
    "- 予測ラベルは one_hot表現ではなく0~9のクラスラベル で表してください。\n",
    "- **下のセルで指定されているx_train、t_train以外の学習データは使わないでください。**\n",
    "- ネットワークの形などは特に制限を設けません。\n",
    "- 高レベルのAPI(tf.layers)を利用しても構いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提出方法\n",
    "\n",
    "- 2つのファイルを提出していただきます。\n",
    "  - テストデータ (x_test) に対する予測ラベルをcsvファイル (ファイル名: submission_pred.csv) で提出してください。\n",
    "  - それに対応するpythonのコードをsubmission_code.pyとして提出してください (%%writefileコマンドなどを利用してください)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価方法\n",
    "\n",
    "- 予測ラベルのt_testに対する精度 (Accuracy) で評価します。\n",
    "- 毎日夜24時にテストデータの一部に対する精度でLeader Boardを更新します。\n",
    "- 締切日の夜24時にテストデータ全体に対する精度でLeader Boardを更新します。これを最終的な評価とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み\n",
    "\n",
    "- この部分は修正しないでください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_cifar10():\n",
    "    \n",
    "    # 学習データ\n",
    "    x_train = np.load('/root/userspace/public/chap07/data/x_train.npy')\n",
    "    t_train = np.load('/root/userspace/public/chap07/data/t_train.npy')\n",
    "\n",
    "    # テストデータ\n",
    "    x_test = np.load('/root/userspace/public/chap07/data/x_test.npy')\n",
    "    \n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    \n",
    "    t_train = np.eye(10)[t_train.astype('int32').flatten()]\n",
    "    \n",
    "    return (x_train, x_test, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳み込みニューラルネットワーク(CNN)の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用するデータの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Flatten, the shape of h is:(?, 1, 1, 512)\n",
      "After Flatten, the shape of h is:(?, 512)\n",
      "EPOCH:1, Valid_Cost:2.216, Valid_Accuracy:0.136\n",
      "EPOCH:2, Valid_Cost:1.875, Valid_Accuracy:0.207\n",
      "EPOCH:3, Valid_Cost:1.763, Valid_Accuracy:0.258\n",
      "EPOCH:4, Valid_Cost:1.682, Valid_Accuracy:0.288\n",
      "EPOCH:5, Valid_Cost:1.654, Valid_Accuracy:0.305\n",
      "EPOCH:6, Valid_Cost:1.604, Valid_Accuracy:0.336\n",
      "EPOCH:7, Valid_Cost:1.535, Valid_Accuracy:0.364\n",
      "EPOCH:8, Valid_Cost:1.499, Valid_Accuracy:0.389\n",
      "EPOCH:9, Valid_Cost:1.484, Valid_Accuracy:0.387\n",
      "EPOCH:10, Valid_Cost:1.455, Valid_Accuracy:0.394\n",
      "EPOCH:11, Valid_Cost:1.428, Valid_Accuracy:0.437\n",
      "EPOCH:12, Valid_Cost:1.319, Valid_Accuracy:0.487\n",
      "EPOCH:13, Valid_Cost:1.228, Valid_Accuracy:0.532\n",
      "EPOCH:14, Valid_Cost:1.206, Valid_Accuracy:0.560\n",
      "EPOCH:15, Valid_Cost:1.123, Valid_Accuracy:0.602\n",
      "EPOCH:16, Valid_Cost:1.038, Valid_Accuracy:0.637\n",
      "EPOCH:17, Valid_Cost:1.033, Valid_Accuracy:0.663\n",
      "EPOCH:18, Valid_Cost:0.938, Valid_Accuracy:0.704\n",
      "EPOCH:19, Valid_Cost:0.931, Valid_Accuracy:0.711\n",
      "EPOCH:20, Valid_Cost:0.952, Valid_Accuracy:0.717\n",
      "EPOCH:21, Valid_Cost:0.975, Valid_Accuracy:0.733\n",
      "EPOCH:22, Valid_Cost:0.872, Valid_Accuracy:0.736\n",
      "EPOCH:23, Valid_Cost:0.915, Valid_Accuracy:0.745\n",
      "EPOCH:24, Valid_Cost:0.898, Valid_Accuracy:0.752\n",
      "EPOCH:25, Valid_Cost:0.906, Valid_Accuracy:0.762\n",
      "EPOCH:26, Valid_Cost:0.896, Valid_Accuracy:0.760\n",
      "EPOCH:27, Valid_Cost:0.906, Valid_Accuracy:0.755\n",
      "EPOCH:28, Valid_Cost:0.896, Valid_Accuracy:0.776\n",
      "EPOCH:29, Valid_Cost:0.847, Valid_Accuracy:0.786\n",
      "EPOCH:30, Valid_Cost:0.786, Valid_Accuracy:0.776\n",
      "EPOCH:31, Valid_Cost:0.960, Valid_Accuracy:0.786\n",
      "EPOCH:32, Valid_Cost:0.860, Valid_Accuracy:0.783\n",
      "EPOCH:33, Valid_Cost:0.902, Valid_Accuracy:0.796\n",
      "EPOCH:34, Valid_Cost:0.815, Valid_Accuracy:0.796\n",
      "EPOCH:35, Valid_Cost:0.851, Valid_Accuracy:0.806\n",
      "EPOCH:36, Valid_Cost:0.927, Valid_Accuracy:0.806\n",
      "EPOCH:37, Valid_Cost:0.976, Valid_Accuracy:0.804\n",
      "EPOCH:38, Valid_Cost:0.919, Valid_Accuracy:0.811\n",
      "EPOCH:39, Valid_Cost:0.959, Valid_Accuracy:0.810\n",
      "EPOCH:40, Valid_Cost:1.050, Valid_Accuracy:0.811\n",
      "EPOCH:41, Valid_Cost:1.001, Valid_Accuracy:0.811\n",
      "EPOCH:42, Valid_Cost:0.905, Valid_Accuracy:0.814\n",
      "EPOCH:43, Valid_Cost:0.945, Valid_Accuracy:0.820\n",
      "EPOCH:44, Valid_Cost:0.972, Valid_Accuracy:0.822\n",
      "EPOCH:45, Valid_Cost:0.759, Valid_Accuracy:0.810\n",
      "EPOCH:46, Valid_Cost:0.981, Valid_Accuracy:0.830\n",
      "EPOCH:47, Valid_Cost:0.897, Valid_Accuracy:0.820\n",
      "EPOCH:48, Valid_Cost:0.776, Valid_Accuracy:0.815\n",
      "EPOCH:49, Valid_Cost:1.018, Valid_Accuracy:0.821\n",
      "EPOCH:50, Valid_Cost:0.853, Valid_Accuracy:0.825\n",
      "EPOCH:51, Valid_Cost:0.942, Valid_Accuracy:0.820\n",
      "EPOCH:52, Valid_Cost:1.079, Valid_Accuracy:0.826\n",
      "EPOCH:53, Valid_Cost:0.998, Valid_Accuracy:0.826\n",
      "EPOCH:54, Valid_Cost:1.003, Valid_Accuracy:0.823\n",
      "EPOCH:55, Valid_Cost:0.973, Valid_Accuracy:0.829\n",
      "EPOCH:56, Valid_Cost:1.095, Valid_Accuracy:0.836\n",
      "EPOCH:57, Valid_Cost:1.079, Valid_Accuracy:0.828\n",
      "EPOCH:58, Valid_Cost:1.031, Valid_Accuracy:0.833\n",
      "EPOCH:59, Valid_Cost:0.956, Valid_Accuracy:0.822\n",
      "EPOCH:60, Valid_Cost:1.156, Valid_Accuracy:0.819\n",
      "EPOCH:61, Valid_Cost:1.044, Valid_Accuracy:0.832\n",
      "EPOCH:62, Valid_Cost:1.002, Valid_Accuracy:0.830\n",
      "EPOCH:63, Valid_Cost:1.124, Valid_Accuracy:0.831\n",
      "EPOCH:64, Valid_Cost:1.018, Valid_Accuracy:0.830\n",
      "EPOCH:65, Valid_Cost:1.155, Valid_Accuracy:0.829\n",
      "EPOCH:66, Valid_Cost:1.116, Valid_Accuracy:0.840\n",
      "EPOCH:67, Valid_Cost:1.082, Valid_Accuracy:0.837\n",
      "EPOCH:68, Valid_Cost:1.159, Valid_Accuracy:0.839\n",
      "EPOCH:69, Valid_Cost:1.219, Valid_Accuracy:0.829\n",
      "EPOCH:70, Valid_Cost:1.011, Valid_Accuracy:0.836\n",
      "EPOCH:71, Valid_Cost:1.018, Valid_Accuracy:0.838\n",
      "EPOCH:72, Valid_Cost:1.003, Valid_Accuracy:0.832\n",
      "EPOCH:73, Valid_Cost:0.993, Valid_Accuracy:0.835\n",
      "EPOCH:74, Valid_Cost:1.229, Valid_Accuracy:0.833\n",
      "EPOCH:75, Valid_Cost:1.162, Valid_Accuracy:0.834\n",
      "EPOCH:76, Valid_Cost:1.080, Valid_Accuracy:0.833\n",
      "EPOCH:77, Valid_Cost:1.371, Valid_Accuracy:0.824\n",
      "EPOCH:78, Valid_Cost:1.313, Valid_Accuracy:0.840\n",
      "EPOCH:79, Valid_Cost:1.363, Valid_Accuracy:0.825\n",
      "EPOCH:80, Valid_Cost:1.238, Valid_Accuracy:0.829\n",
      "EPOCH:81, Valid_Cost:1.288, Valid_Accuracy:0.832\n",
      "EPOCH:82, Valid_Cost:1.145, Valid_Accuracy:0.835\n",
      "EPOCH:83, Valid_Cost:1.199, Valid_Accuracy:0.840\n",
      "EPOCH:84, Valid_Cost:1.334, Valid_Accuracy:0.838\n",
      "EPOCH:85, Valid_Cost:1.258, Valid_Accuracy:0.828\n",
      "EPOCH:86, Valid_Cost:1.226, Valid_Accuracy:0.830\n",
      "EPOCH:87, Valid_Cost:1.476, Valid_Accuracy:0.838\n",
      "EPOCH:88, Valid_Cost:1.409, Valid_Accuracy:0.833\n",
      "EPOCH:89, Valid_Cost:1.248, Valid_Accuracy:0.836\n",
      "EPOCH:90, Valid_Cost:1.096, Valid_Accuracy:0.834\n",
      "EPOCH:91, Valid_Cost:1.083, Valid_Accuracy:0.842\n",
      "EPOCH:92, Valid_Cost:1.305, Valid_Accuracy:0.834\n",
      "EPOCH:93, Valid_Cost:1.310, Valid_Accuracy:0.838\n",
      "EPOCH:94, Valid_Cost:1.352, Valid_Accuracy:0.846\n",
      "EPOCH:95, Valid_Cost:1.290, Valid_Accuracy:0.830\n",
      "EPOCH:96, Valid_Cost:1.520, Valid_Accuracy:0.836\n",
      "EPOCH:97, Valid_Cost:1.292, Valid_Accuracy:0.827\n",
      "EPOCH:98, Valid_Cost:1.362, Valid_Accuracy:0.842\n",
      "EPOCH:99, Valid_Cost:1.328, Valid_Accuracy:0.849\n",
      "EPOCH:100, Valid_Cost:1.278, Valid_Accuracy:0.844\n"
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap07/submission/submission_code_VGG.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "\n",
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))\n",
    "\n",
    "### layer定義 ###\n",
    "\n",
    "class Conv:\n",
    "    def __init__(self, filter_shape, function = lambda x: x, strides = [1,1,1,1], padding = 'VALID'):\n",
    "        # He initializationを使う\n",
    "        # filter_shape = Height * Width * Num of input_channels * Num of output_channels\n",
    "        fun_in = np.prod(filter_shape[:3])\n",
    "        fun_out = np.prod(filter_shape[:2]) * filter_shape[3]\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = -np.sqrt(6/ fun_in),\n",
    "                high = np.sqrt(6/ fun_out),\n",
    "                size = filter_shape\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((filter_shape[3]), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        u = tf.nn.conv2d(x, self.W, strides = self.strides, padding = self.padding) + self.b\n",
    "        return self.function(u)\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, shape, epsilon=np.float32(1e-5)):\n",
    "        self.gamma = tf.Variable(np.ones(shape, dtype='float32'), name='gamma')\n",
    "        self.beta  = tf.Variable(np.zeros(shape, dtype='float32'), name='beta')\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mean, var = tf.nn.moments(x, axes=(0,1,2), keep_dims=True)\n",
    "        std = tf.sqrt(var + self.epsilon)\n",
    "        x_normalized = (x - mean)/std\n",
    "        return self.gamma * x_normalized + self.beta    \n",
    "    \n",
    "class Activation:\n",
    "    def __init__(self, function=lambda x: x):\n",
    "        self.function = function\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.function(x)\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, ksize = [1, 2, 2, 1] , strides = [1, 2, 2, 1], padding = 'VALID'):\n",
    "        self.ksize = ksize\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.max_pool(x, ksize = self.ksize, strides = self.strides, padding = self.padding)\n",
    "    \n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        return tf.reshape(x, (-1, np.prod(x.get_shape().as_list()[1:])))\n",
    "    \n",
    "    \n",
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function = lambda x: x):\n",
    "        # ここでも, He Initialization\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = - np.sqrt(6/ in_dim),\n",
    "                high = np.sqrt(6/ in_dim),\n",
    "                size = [in_dim, out_dim]\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((out_dim), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        u = tf.matmul(x, self.W) + self.b\n",
    "        return self.function(u)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_keep_prob=1.0):\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.params = []\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # 訓練時のみdropoutを適用\n",
    "        return tf.cond(\n",
    "            pred=is_training,\n",
    "            true_fn=lambda: tf.nn.dropout(x, keep_prob=self.dropout_keep_prob),\n",
    "            false_fn=lambda: x\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "### ネットワーク ###\n",
    "tf.reset_default_graph()\n",
    "is_training = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "lmd = 0.0001\n",
    "dropout_keep_prob = 0.75\n",
    "\n",
    "\n",
    "# VGG16 network\n",
    "\n",
    "\n",
    "# Block1\n",
    "h = Conv(filter_shape = (3, 3, 3, 64), function = tf.nn.relu, padding = 'SAME')(x) # [None, 32, 32, 3] -> [None, 32, 32, 64]\n",
    "h = Conv(filter_shape = (3, 3, 64, 64), function = tf.nn.relu, padding = 'SAME')(h) # [None, 32, 32, 64] -> [None, 32, 32, 64]\n",
    "h = BatchNorm(shape = (32,32,64))(h) # [None, 32, 32, 64] -> [None, 32, 32, 64]\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 32, 32, 64] -> [None, 16, 16, 64]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "# Block2\n",
    "h = Conv(filter_shape = (3, 3, 64, 128), function = tf.nn.relu, padding = 'SAME')(h) # [None, 16, 16, 64] -> [None, 16, 16, 128]\n",
    "h = Conv(filter_shape = (3, 3, 128, 128), function = tf.nn.relu, padding = 'SAME')(h) # [None, 16, 16, 128] -> [None, 16, 16, 128]\n",
    "h = BatchNorm(shape = (16, 16, 128))(h)\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 16, 16, 128] -> [None, 8, 8, 128]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "# Block3\n",
    "h = Conv(filter_shape = (3, 3, 128, 256), function = tf.nn.relu, padding = 'SAME')(h) # [None, 8, 8, 128] -> [None, 8, 8, 256]\n",
    "h = Conv(filter_shape = (3, 3, 256, 256), function = tf.nn.relu, padding = 'SAME')(h) # [None, 8, 8, 256] -> [None, 8, 8, 256]\n",
    "h = Conv(filter_shape = (3, 3, 256, 256), function = tf.nn.relu, padding = 'SAME')(h) # [None, 8, 8, 256] -> [None, 8, 8, 256]\n",
    "h = BatchNorm(shape = (8, 8, 256))(h)\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1], padding = 'SAME')(h) # [None, 8, 8, 256] -> [None, 4, 4, 256]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "# Block4\n",
    "h = Conv(filter_shape = (3, 3, 256, 512), function = tf.nn.relu, padding = 'SAME')(h) # [None, 4, 4, 256] -> [None, 4, 4, 512]\n",
    "h = Conv(filter_shape = (3, 3, 512, 512), function = tf.nn.relu, padding = 'SAME')(h) # [None, 4, 4, 512] -> [None, 4, 4, 512]\n",
    "h = Conv(filter_shape = (3, 3, 512, 512), function = tf.nn.relu, padding = 'SAME')(h) # [None, 4, 4, 512] -> [None, 4, 4, 512]\n",
    "h = BatchNorm(shape = (4, 4, 512))(h)\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1], padding = 'SAME')(h) # [None, 4, 4, 512] -> [None, 2, 2, 512]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "# Block5\n",
    "h = Conv(filter_shape = (3, 3, 512, 512), function = tf.nn.relu, padding = 'SAME')(h) # [None, 2, 2, 512] -> [None, 2, 2, 512]\n",
    "h = Conv(filter_shape = (3, 3, 512, 512), function = tf.nn.relu, padding = 'SAME')(h) # [None, 2, 2, 512] -> [None, 2, 2, 512]\n",
    "h = Conv(filter_shape = (3, 3, 512, 512), function = tf.nn.relu, padding = 'SAME')(h) # [None, 2, 2, 512] -> [None, 2, 2, 512]\n",
    "h = BatchNorm(shape = (2, 2, 512))(h)\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1], padding = 'SAME')(h) # [None, 2, 2, 512] -> [None, 1, 1, 512]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "\n",
    "\n",
    "print(\"Before Flatten, the shape of h is:{}\".format(h.shape))\n",
    "h = Flatten()(h)\n",
    "print(\"After Flatten, the shape of h is:{}\".format(h.shape))\n",
    "\n",
    "h = Dense(512, 200, tf.nn.relu)(h)\n",
    "h = Dropout(0.5)(h)\n",
    "y = Dense(200, 10, tf.nn.softmax)(h)\n",
    "\n",
    "\n",
    "cost = - tf.reduce_mean(tf.reduce_sum(t * tf_log(y), axis=1))\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "# optimizer = tf.train.AdadeltaOptimizer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "### 元々あった記述 ###\n",
    "# cost = - tf.reduce_mean(tf.reduce_sum(t * tf_log(y), axis=1))\n",
    "# update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# with tf.control_dependencies(update_ops):\n",
    "#     optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "### 前処理 ###\n",
    "def gcn(x, epsilon = 1e-8):\n",
    "    mean = np.mean(x, axis = (1, 2, 3), keepdims = True)\n",
    "    var = np.var(x, axis = (1, 2, 3), keepdims = True)\n",
    "    return (x - mean)/np.sqrt(var + epsilon)\n",
    "\n",
    "\n",
    "class ZCAWhitening:\n",
    "    def __init__(self, epsilon = 1e-4):\n",
    "        self.epsilon = epsilon\n",
    "        self.mean = None\n",
    "        self.ZCA_matrix = None\n",
    "        \n",
    "    def fit(self, x):\n",
    "        x = x.reshape(x.shape[0], -1) # 1枚の画像を１つのベクトルにしてしまう\n",
    "        self.mean = np.mean(x, axis = 0) # 全ての画像のchannel kの(i,j)成分毎に平均を取る。\n",
    "        x -= self.mean # 標準化\n",
    "        cov_matrix = np.matmul(x.T, x)/x.shape[0] # 共分散行列の推定値\n",
    "        A, d, _ = np.linalg.svd(cov_matrix)\n",
    "        self.ZCA_matrix = np.matmul(A, np.matmul(np.diag(1./ np.sqrt(d + self.epsilon)), A.T))\n",
    "        \n",
    "    def transform(self, x):\n",
    "        shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x -= self.mean\n",
    "        x = np.dot(x, self.ZCA_matrix.T) # ここの積の形に注意。\n",
    "        return x.reshape(shape)\n",
    "\n",
    "    \n",
    "x_train, x_test, t_train = load_cifar10()\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.1, random_state=random_state)\n",
    "zca = ZCAWhitening()\n",
    "zca.fit(x_train)\n",
    "x_train_zca = zca.transform(gcn(x_train))\n",
    "t_train_zca = t_train[:]\n",
    "x_valid_zca = zca.transform(gcn(x_valid))\n",
    "t_valid_zca = t_valid[:]\n",
    "x_test_zca = zca.transform(gcn(x_test))\n",
    "\n",
    "### 学習 ###\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "n_batches = x_train.shape[0]//batch_size\n",
    "\n",
    "# sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        x_train, t_train = shuffle(x_train_zca, t_train_zca, random_state = random_state)\n",
    "        for batch in range(n_batches):\n",
    "            start = batch*batch_size\n",
    "            finish = start + batch_size\n",
    "            sess.run(train, feed_dict = {\n",
    "                x:x_train_zca[start:finish], \n",
    "                t:t_train_zca[start:finish],\n",
    "                is_training:True,\n",
    "            })\n",
    "        _pred, _cost = sess.run([y, cost], feed_dict = {\n",
    "            x:x_valid_zca,\n",
    "            t:t_valid_zca,\n",
    "            is_training:False,\n",
    "        })\n",
    "        print(\"EPOCH:{}, Valid_Cost:{:.3f}, Valid_Accuracy:{:.3f}\".format(\n",
    "            epoch + 1,\n",
    "            _cost,\n",
    "            accuracy_score(t_valid_zca.argmax(axis = 1), _pred.argmax(axis = 1))\n",
    "        ))\n",
    "#         if (accuracy_score(t_valid_zca.argmax(axis = 1), _pred.argmax(axis = 1)) > 0.84):\n",
    "#             print(\"The target accuracy is achieved! Learning process is stopping...\")\n",
    "#             break\n",
    "    del x_train_zca, x_valid_zca, t_train_zca, t_valid_zca\n",
    "    y_pred = []\n",
    "    for i in range(10):\n",
    "        start = i*1000\n",
    "        finish = start + 1000\n",
    "        _pred = sess.run(y, feed_dict = {x:x_test_zca[start:finish], is_training:False})\n",
    "        y_pred = np.concatenate([y_pred,_pred.argmax(axis = 1)])\n",
    "    submission = pd.Series(y_pred, name='label')\n",
    "    submission.to_csv('/root/userspace/chap07/submission/submission_pred_VGG16_100epochs.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_zca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
