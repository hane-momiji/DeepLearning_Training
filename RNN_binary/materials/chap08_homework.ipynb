{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "RNNを用いてIMDbのsentiment analysisを実装してみましょう。\n",
    "\n",
    "ネットワークの形などは特に制限を設けませんし、今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目標値\n",
    "F値：0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ルール\n",
    "- 以下のセルで指定されている`x_train, t_train`以外の学習データは使わないでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提出方法\n",
    "- 2つのファイルを提出していただきます。\n",
    "  1. テストデータ (x_test) に対する予測ラベルを`submission_pred.csv`として保存し、Homeworkタブから**chap08**を選択して提出してください。\n",
    "  2. それに対応するpythonのコードを`submission_code.py`として保存し、Homeworkタブから**chap08 (code)**を選択して提出してください。\n",
    "    - セルに書いたコードを.py形式で保存するためには%%writefileコマンドなどを利用してください。\n",
    "    - writefileコマンドではファイルの保存のみが行われセル内のpythonコード自体は実行されません。そのため、実際にコードを走らせる際にはwritefileコマンドをコメントアウトしてください\n",
    "\n",
    "\n",
    "- コードの内容を変更した場合は、1と2の両方を提出し直してください。\n",
    "\n",
    "- なお、採点は1で行い、2はコードの確認用として利用します。(成績優秀者はコード内容を公開させていただくかもしれません。)\n",
    "\n",
    "- **宿題の締め切りは【出題週の翌週水曜日24時】です。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価方法\n",
    "\n",
    "- 予測ラベルの（`t_test`に対する）F値で評価します。\n",
    "- 毎日24時にテストデータの一部に対するF値でLeader Boardを更新します。\n",
    "- 締切日の夜24時にテストデータ全体に対するF値でLeader Boardを更新します。これを最終的な評価とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み（このセルは修正しないでください）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset():\n",
    "    # 学習データ\n",
    "    x_train = np.load('/root/userspace/public/chap08/data/x_train.npy')\n",
    "    t_train = np.load('/root/userspace/public/chap08/data/t_train.npy')\n",
    "    \n",
    "    # テストデータ\n",
    "    x_test = np.load('/root/userspace/public/chap08/data/x_test.npy')\n",
    "\n",
    "    return (x_train, x_test, t_train)\n",
    "\n",
    "x_train, x_test, t_train = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-fa2203c47a4c>:21: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "EPOCH: 1, Training Cost: 0.529, Validation Cost: 0.555, Validation F1: 0.709\n",
      "EPOCH: 2, Training Cost: 0.452, Validation Cost: 0.647, Validation F1: 0.600\n",
      "EPOCH: 3, Training Cost: 0.470, Validation Cost: 0.451, Validation F1: 0.810\n",
      "EPOCH: 4, Training Cost: 0.291, Validation Cost: 0.467, Validation F1: 0.799\n",
      "EPOCH: 5, Training Cost: 0.340, Validation Cost: 0.585, Validation F1: 0.716\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fa2203c47a4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m### 出力 ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/userspace/chap08/submission/submission_pred.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap08/materials/submission_code.py\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "### 関数定義 ###\n",
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))\n",
    "\n",
    "### レイヤー定義 ###\n",
    "class Embedding:\n",
    "    def __init__(self, vocab_size, emb_dim, scale = 0.08):\n",
    "        self.V = tf.Variable(tf.random_normal(shape = [vocab_size, emb_dim], stddev = scale), name = \"V\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.embedding_lookup(self.V, x)\n",
    "    \n",
    "class RNN:\n",
    "    def __init__(self, hid_dim, seq_len = None, initial_state = None):\n",
    "        self.cell = tf.nn.rnn_cell.BasicRNNCell(hid_dim)\n",
    "        self.initial_state = initial_state\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.initial_state is None:\n",
    "            self.initial_state = self.cell.zero_state(tf.shape(x)[0], tf.float32)\n",
    "            \n",
    "        # outputsは各系列長分以降は0になるので注意\n",
    "        outputs, state = tf.nn.dynamic_rnn(self.cell, x, self.seq_len, self.initial_state)\n",
    "        return tf.gather_nd(outputs, indices = tf.stack([tf.range(tf.shape(x)[0]), self.seq_len-1], axis = 1 ))\n",
    "\n",
    "### グラフ構築 ###\n",
    "tf.reset_default_graph()\n",
    "\n",
    "emb_dim = 100\n",
    "hid_dim = 50\n",
    "# np.hstackで横方向に並べるので謎だが、要は\n",
    "# x_train, x_test全てのなかで最もidが大きい単語は何かを抽出している?\n",
    "num_words = max([max(s) for s in np.hstack((x_train, x_test))])\n",
    "pad_index = 0\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "t = tf.placeholder(tf.float32, [None, None], name='t')\n",
    "\n",
    "seq_len = tf.reduce_sum(tf.cast(tf.not_equal(x, pad_index), tf.int32), axis=1)\n",
    "\n",
    "h = Embedding(num_words, emb_dim)(x)\n",
    "h = RNN(hid_dim, seq_len)(h)\n",
    "y = tf.layers.Dense(1, tf.nn.sigmoid)(h)\n",
    "\n",
    "cost = -tf.reduce_mean(t*tf_log(y) + (1 - t)*tf_log(1 - y))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "test = tf.round(y)\n",
    "\n",
    "### データの準備 ###\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train)\n",
    "\n",
    "### 学習 ###\n",
    "n_epochs = 5\n",
    "batch_size = 100 # バッチサイズが大きいと、ResourceExhaustedErrorになることがあります\n",
    "n_batches_train = len(x_train) // batch_size\n",
    "n_batches_valid = len(x_valid) // batch_size\n",
    "n_batches_test = len(x_test) // batch_size\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_costs = []\n",
    "        for i in range(n_batches_train):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            x_train_batch = np.array(pad_sequences(x_train[start:end], padding='post', value=pad_index))\n",
    "            t_train_batch = np.array(t_train[start:end])[:, None]\n",
    "\n",
    "            _, train_cost = sess.run([train, cost], feed_dict={x: x_train_batch, t: t_train_batch})\n",
    "            train_costs.append(train_cost)\n",
    "        \n",
    "        # Valid\n",
    "        valid_costs = []\n",
    "        y_pred = []\n",
    "        for i in range(n_batches_valid):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            x_valid_pad = np.array(pad_sequences(x_valid[start:end], padding='post', value=pad_index))\n",
    "            t_valid_pad = np.array(t_valid[start:end])[:, None]\n",
    "            \n",
    "            pred, valid_cost = sess.run([test, cost], feed_dict={x: x_valid_pad, t: t_valid_pad})\n",
    "            y_pred += pred.flatten().tolist()\n",
    "            valid_costs.append(valid_cost)\n",
    "        print('EPOCH: {}, Training Cost: {:.3f}, Validation Cost: {:.3f}, Validation F1: {:.3f}'.format(epoch+1, np.mean(train_costs), np.mean(valid_costs), f1_score(t_valid, y_pred, average='macro')))\n",
    "    \n",
    "    # Test\n",
    "    test_costs = []\n",
    "    test_y_pred = []\n",
    "    for i in range(n_batches_test):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        x_test_pad = np.array(pad_sequences(x_test[start:end], padding = 'post', value = pad_index))\n",
    "        _pred = sess.run(test, feed_dict = {x:x_test_pad})\n",
    "        test_y_pred += _pred.flatten().tolist()\n",
    "        \n",
    "    ### 出力 ###\n",
    "    submission = pd.Series(test_y_pred, name='label')\n",
    "    submission.to_csv('/root/userspace/chap08/submission/submission_pred.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(t_valid.shape)\n",
    "type(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-83ade75ceb19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mt_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt_train_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mtrain_costs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap08/submission/submission_code_LSTM.py\n",
    "np.random.seed(34)\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "### 関数定義 ###\n",
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))\n",
    "\n",
    "### レイヤー定義 ###\n",
    "class Embedding:\n",
    "    def __init__(self, vocab_size, emb_dim, scale = 0.08):\n",
    "        self.V = tf.Variable(tf.random_normal(shape = [vocab_size, emb_dim], stddev = scale), name = \"V\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.embedding_lookup(self.V, x)\n",
    "    \n",
    "class RNN:\n",
    "    def __init__(self, hid_dim, seq_len = None, initial_state = None):\n",
    "        self.cell = tf.nn.rnn_cell.BasicRNNCell(hid_dim)\n",
    "        self.initial_state = initial_state\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.initial_state is None:\n",
    "            self.initial_state = self.cell.zero_state(tf.shape(x)[0], tf.float32)\n",
    "            \n",
    "        # outputsは各系列長分以降は0になるので注意\n",
    "        outputs, state = tf.nn.dynamic_rnn(self.cell, x, self.seq_len, self.initial_state)\n",
    "        return tf.gather_nd(outputs, indices = tf.stack([tf.range(tf.shape(x)[0]), self.seq_len-1], axis = 1 ))\n",
    "    \n",
    "### LSTMによるレイヤー定義 ###\n",
    "class LSTM:\n",
    "    def __init__(self, in_dim, hid_dim, seq_len = None, initial_state = None):\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        glorot = tf.cast(tf.sqrt(6/(in_dim + hid_dim*2)), tf.float32)\n",
    "        \n",
    "        # 入力ゲート\n",
    "        self.W_i = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_i')\n",
    "        self.b_i  = tf.Variable(tf.zeros([hid_dim]), name='b_i')\n",
    "        \n",
    "        # 忘却ゲート\n",
    "        self.W_f = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_f')\n",
    "        self.b_f  = tf.Variable(tf.zeros([hid_dim]), name='b_f')\n",
    "\n",
    "        # 出力ゲート\n",
    "        self.W_o = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_o')\n",
    "        self.b_o  = tf.Variable(tf.zeros([hid_dim]), name='b_o')\n",
    "\n",
    "        # セル\n",
    "        self.W_c = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_c')\n",
    "        self.b_c  = tf.Variable(tf.zeros([hid_dim]), name='b_c')\n",
    "\n",
    "        # マスク\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.initial_state = initial_state\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # tf.scanへの適用関数fn\n",
    "        # WRITE ME\n",
    "        def fn(prev_state, x_and_m):\n",
    "            x_t, m_t = x_and_m\n",
    "            c_prev, h_prev = prev_state[0], prev_state[1]\n",
    "            inputs = tf.concat([x_t, h_prev], axis = -1)\n",
    "            \n",
    "            # 入力ゲート\n",
    "            i_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_i) + self.b_i)\n",
    "            # 忘却ゲート\n",
    "            f_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_f) + self.b_f)\n",
    "            # 出力ゲート\n",
    "            o_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_o) + self.b_o)\n",
    "            \n",
    "            # セル\n",
    "            c_t = tf.multiply(f_t, c_prev) + tf.nn.tanh(tf.matmul(inputs, self.W_c) + self.b_c)\n",
    "            # 隠れ層\n",
    "            h_t = tf.multiply(o_t, tf.nn.tanh(c_t))\n",
    "            \n",
    "            # マスクの適用\n",
    "            c_t = m_t*c_t + (1 - m_t)*c_prev\n",
    "            h_t = m_t*h_t + (1 - m_t)*h_prev\n",
    "            # 出力\n",
    "            return tf.stack([c_t, h_t])\n",
    "\n",
    "        # 入力の時間順化\n",
    "        x_tmaj = tf.transpose(x, perm=[1, 0, 2])\n",
    "        \n",
    "        # マスクの生成＆時間順化\n",
    "        mask = tf.cast(tf.sequence_mask(self.seq_len, tf.shape(x)[1]), tf.float32)\n",
    "        mask_tmaj = tf.transpose(tf.expand_dims(mask, axis=-1), perm=[1, 0, 2])\n",
    "        \n",
    "        if self.initial_state is None:\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            self.initial_state = tf.stack([tf.zeros([batch_size, self.hid_dim]), tf.zeros([batch_size, self.hid_dim])])\n",
    "\n",
    "        state_seq = tf.scan(fn=fn, elems=[x_tmaj, mask_tmaj], initializer=self.initial_state)\n",
    "        \n",
    "        return state_seq[-1][1]\n",
    "\n",
    "### グラフ構築 ###\n",
    "tf.reset_default_graph()\n",
    "\n",
    "emb_dim = 32\n",
    "hid_dim = 100\n",
    "# np.hstackで横方向に並べるので謎だが、要は\n",
    "# x_train, x_test全てのなかで最もidが大きい単語は何かを抽出している?\n",
    "num_words = max([max(s) for s in np.hstack((x_train, x_test))])\n",
    "pad_index = 0\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "t = tf.placeholder(tf.float32, [None, None], name='t')\n",
    "\n",
    "seq_len = tf.reduce_sum(tf.cast(tf.not_equal(x, pad_index), tf.int32), axis=1)\n",
    "\n",
    "h = Embedding(num_words, emb_dim)(x)\n",
    "# h = RNN(hid_dim, seq_len)(h)\n",
    "h = LSTM(emb_dim, hid_dim, seq_len)(h)\n",
    "y = tf.layers.Dense(1, tf.nn.sigmoid)(h)\n",
    "\n",
    "cost = -tf.reduce_mean(t*tf_log(y) + (1 - t)*tf_log(1 - y))\n",
    "\n",
    "# train = tf.train.AdamOptimizer().minimize(cost) を以下に置き換え\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "# grads = optimizer.compute_gradients(cost)\n",
    "# clipped_grads = [(tf.clip_by_value(grad_val, -1., 1.), var) for grad_val, var in grads]\n",
    "# train = optimizer.apply_gradients(clipped_grads)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "test = tf.round(y)\n",
    "\n",
    "### データの準備 ###\n",
    "top_words = 5000 # We only use most frequent 5000 words\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train)\n",
    "\n",
    "### 頻度の高い単語のみを抽出 ###\n",
    "def freq_words(x, top_words = 5000):\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        x[i] = [j for j in x[i] if j <= top_words]\n",
    "    return x\n",
    "\n",
    "### 抽出された軽いデータ ###\n",
    "x_train_freq = freq_words(x_train)\n",
    "x_valid_freq = freq_words(x_valid)\n",
    "x_test_freq = freq_words(x_test)\n",
    "\n",
    "\n",
    "### 学習 ###\n",
    "n_epochs = 3\n",
    "batch_size = 100 # バッチサイズが大きいと、ResourceExhaustedErrorになることがあります\n",
    "n_batches_train = math.ceil(len(x_train) / batch_size)\n",
    "n_batches_valid = math.ceil(len(x_valid) / batch_size)\n",
    "n_batches_test = math.ceil(len(x_test) / batch_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_costs = []\n",
    "        for i in range(n_batches_train):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, len(x_train))\n",
    "            \n",
    "            # x_train_batch = np.array(pad_sequences(x_train[start:end], padding='post', value=pad_index))\n",
    "            x_train_batch = np.array(pad_sequences(x_train_freq[start:end], padding='post', value=pad_index))\n",
    "            t_train_batch = np.array(t_train[start:end])[:, None]\n",
    "\n",
    "            _, train_cost = sess.run([train, cost], feed_dict={x: x_train_batch, t: t_train_batch})\n",
    "            train_costs.append(train_cost)\n",
    "        \n",
    "        # Valid\n",
    "        valid_costs = []\n",
    "        y_pred = []\n",
    "        for i in range(n_batches_valid):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, len(x_valid))\n",
    "            # x_valid_pad = np.array(pad_sequences(x_valid[start:end], padding='post', value=pad_index))\n",
    "            x_valid_pad = np.array(pad_sequences(x_valid_freq[start:end], padding='post', value=pad_index))\n",
    "            t_valid_pad = np.array(t_valid[start:end])[:, None]\n",
    "            \n",
    "            pred, valid_cost = sess.run([test, cost], feed_dict={x: x_valid_pad, t: t_valid_pad})\n",
    "            y_pred += pred.flatten().tolist()\n",
    "            valid_costs.append(valid_cost)\n",
    "        print('EPOCH: {}, Training Cost: {:.3f}, Validation Cost: {:.3f}, Validation F1: {:.3f}'.format(epoch+1, np.mean(train_costs), np.mean(valid_costs), f1_score(t_valid, y_pred, average='macro')))\n",
    "        if (f1_score(t_valid, y_pred, average='macro')>0.85):\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # Test\n",
    "    test_costs = []\n",
    "    test_y_pred = []\n",
    "    for i in range(n_batches_test):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        # x_test_pad = np.array(pad_sequences(x_test[start:end], padding = 'post', value = pad_index))\n",
    "        x_test_pad = np.array(pad_sequences(x_test_freq[start:end], padding = 'post', value = pad_index))\n",
    "        _pred = sess.run(test, feed_dict = {x:x_test_pad})\n",
    "        test_y_pred += _pred.flatten().tolist()\n",
    "        \n",
    "    ### 出力 ###\n",
    "    submission = pd.Series(test_y_pred, name='label')\n",
    "    submission.to_csv('/root/userspace/chap08/submission/submission_pred_LSTM_freq.csv', header=True, index_label='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Training Cost: 0.693, Validation Cost: 0.693, Validation F1: 0.336\n",
      "EPOCH: 2, Training Cost: 0.693, Validation Cost: 0.693, Validation F1: 0.350\n",
      "EPOCH: 3, Training Cost: 0.693, Validation Cost: 0.693, Validation F1: 0.340\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7275d5c0375c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mt_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt_train_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mtrain_costs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap08/submission/submission_code_LSTM.py\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "### 関数定義 ###\n",
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))\n",
    "\n",
    "### レイヤー定義 ###\n",
    "class Embedding:\n",
    "    def __init__(self, vocab_size, emb_dim, scale = 0.08):\n",
    "        self.V = tf.Variable(tf.random_normal(shape = [vocab_size, emb_dim], stddev = scale), name = \"V\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.embedding_lookup(self.V, x)\n",
    "    \n",
    "class RNN:\n",
    "    def __init__(self, hid_dim, seq_len = None, initial_state = None):\n",
    "        self.cell = tf.nn.rnn_cell.BasicRNNCell(hid_dim)\n",
    "        self.initial_state = initial_state\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.initial_state is None:\n",
    "            self.initial_state = self.cell.zero_state(tf.shape(x)[0], tf.float32)\n",
    "            \n",
    "        # outputsは各系列長分以降は0になるので注意\n",
    "        outputs, state = tf.nn.dynamic_rnn(self.cell, x, self.seq_len, self.initial_state)\n",
    "        return tf.gather_nd(outputs, indices = tf.stack([tf.range(tf.shape(x)[0]), self.seq_len-1], axis = 1 ))\n",
    "    \n",
    "### LSTMによるレイヤー定義 ###\n",
    "class LSTM:\n",
    "    def __init__(self, in_dim, hid_dim, seq_len = None, initial_state = None):\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        glorot = tf.cast(tf.sqrt(6/(in_dim + hid_dim*2)), tf.float32)\n",
    "        \n",
    "        # 入力ゲート\n",
    "        self.W_i = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_i')\n",
    "        self.b_i  = tf.Variable(tf.zeros([hid_dim]), name='b_i')\n",
    "        \n",
    "        # 忘却ゲート\n",
    "        self.W_f = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_f')\n",
    "        self.b_f  = tf.Variable(tf.zeros([hid_dim]), name='b_f')\n",
    "\n",
    "        # 出力ゲート\n",
    "        self.W_o = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_o')\n",
    "        self.b_o  = tf.Variable(tf.zeros([hid_dim]), name='b_o')\n",
    "\n",
    "        # セル\n",
    "        self.W_c = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_c')\n",
    "        self.b_c  = tf.Variable(tf.zeros([hid_dim]), name='b_c')\n",
    "\n",
    "        # マスク\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.initial_state = initial_state\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # tf.scanへの適用関数fn\n",
    "        # WRITE ME\n",
    "        def fn(prev_state, x_and_m):\n",
    "            x_t, m_t = x_and_m\n",
    "            c_prev, h_prev = prev_state[0], prev_state[1]\n",
    "            inputs = tf.concat([x_t, h_prev], axis = -1)\n",
    "            \n",
    "            # 入力ゲート\n",
    "            i_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_i) + self.b_i)\n",
    "            # 忘却ゲート\n",
    "            f_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_f) + self.b_f)\n",
    "            # 出力ゲート\n",
    "            o_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_o) + self.b_o)\n",
    "            \n",
    "            # セル\n",
    "            c_t = tf.multiply(f_t, c_prev) + tf.nn.tanh(tf.matmul(inputs, self.W_c) + self.b_c)\n",
    "            # 隠れ層\n",
    "            h_t = tf.multiply(o_t, tf.nn.tanh(c_t))\n",
    "            \n",
    "            # マスクの適用\n",
    "            c_t = m_t*c_t + (1 - m_t)*c_prev\n",
    "            h_t = m_t*h_t + (1 - m_t)*h_prev\n",
    "            # 出力\n",
    "            return tf.stack([c_t, h_t])\n",
    "\n",
    "        # 入力の時間順化\n",
    "        x_tmaj = tf.transpose(x, perm=[1, 0, 2])\n",
    "        \n",
    "        # マスクの生成＆時間順化\n",
    "        mask = tf.cast(tf.sequence_mask(self.seq_len, tf.shape(x)[1]), tf.float32)\n",
    "        mask_tmaj = tf.transpose(tf.expand_dims(mask, axis=-1), perm=[1, 0, 2])\n",
    "        \n",
    "        if self.initial_state is None:\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            self.initial_state = tf.stack([tf.zeros([batch_size, self.hid_dim]), tf.zeros([batch_size, self.hid_dim])])\n",
    "\n",
    "        state_seq = tf.scan(fn=fn, elems=[x_tmaj, mask_tmaj], initializer=self.initial_state)\n",
    "        \n",
    "        return state_seq[-1][1]\n",
    "\n",
    "### 畳み込み層定義 ###\n",
    "class Conv1D:\n",
    "    def __init__(self, filter_shape, function = lambda x: x, strides = 1, padding = 'VALID'):\n",
    "        # He initializationを使う\n",
    "        # filter_shape = Height * Width * Num of input_channels * Num of output_channels\n",
    "        fun_in = np.prod(filter_shape[:3])\n",
    "        fun_out = np.prod(filter_shape[:2]) * filter_shape[3]\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = -np.sqrt(6/ fun_in),\n",
    "                high = np.sqrt(6/ fun_out),\n",
    "                size = filter_shape\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((filter_shape[2]), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        u = tf.nn.conv1d(x, self.W, strides = self.strides, padding = self.padding) + self.b\n",
    "        return self.function(u)\n",
    "\n",
    "### グラフ構築 ###\n",
    "tf.reset_default_graph()\n",
    "\n",
    "emb_dim = 32\n",
    "hid_dim = 100\n",
    "# np.hstackで横方向に並べるので謎だが、要は\n",
    "# x_train, x_test全てのなかで最もidが大きい単語は何かを抽出している?\n",
    "num_words = max([max(s) for s in np.hstack((x_train, x_test))])\n",
    "pad_index = 0\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "t = tf.placeholder(tf.float32, [None, None], name='t')\n",
    "\n",
    "seq_len = tf.reduce_sum(tf.cast(tf.not_equal(x, pad_index), tf.int32), axis=1)\n",
    "\n",
    "h = Embedding(num_words, emb_dim)(x)\n",
    "# h = Conv1D(filter_shape = [3,1,1], padding = \"SAME\")(h)\n",
    "h = tf.layers.Conv1D(filters = 32, kernel_size = 3, strides = 1, padding = \"SAME\")(h)\n",
    "h = tf.layers.MaxPooling1D(pool_size = 2, strides = 2)(h)\n",
    "h = LSTM(emb_dim, hid_dim, seq_len)(h)\n",
    "\n",
    "\n",
    "y = tf.layers.Dense(1, tf.nn.sigmoid)(h)\n",
    "\n",
    "cost = -tf.reduce_mean(t*tf_log(y) + (1 - t)*tf_log(1 - y))\n",
    "\n",
    "# train = tf.train.AdamOptimizer().minimize(cost) を以下に置き換え\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "# grads = optimizer.compute_gradients(cost)\n",
    "# clipped_grads = [(tf.clip_by_value(grad_val, -1., 1.), var) for grad_val, var in grads]\n",
    "# train = optimizer.apply_gradients(clipped_grads)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "test = tf.round(y)\n",
    "\n",
    "### データの準備 ###\n",
    "top_words = 5000 # We only use most frequent 5000 words\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train)\n",
    "\n",
    "### 頻度の高い単語のみを抽出 ###\n",
    "def freq_words(x, top_words = 5000):\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        x[i] = [j for j in x[i] if j <= top_words]\n",
    "    return x\n",
    "\n",
    "### 抽出された軽いデータ ###\n",
    "x_train_freq = freq_words(x_train)\n",
    "x_valid_freq = freq_words(x_valid)\n",
    "x_test_freq = freq_words(x_test)\n",
    "\n",
    "\n",
    "### 学習 ###\n",
    "n_epochs = 5\n",
    "batch_size = 100 # バッチサイズが大きいと、ResourceExhaustedErrorになることがあります\n",
    "n_batches_train = math.ceil(len(x_train) / batch_size)\n",
    "n_batches_valid = math.ceil(len(x_valid) / batch_size)\n",
    "n_batches_test = math.ceil(len(x_test) / batch_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_costs = []\n",
    "        for i in range(n_batches_train):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, len(x_train))\n",
    "            \n",
    "            # x_train_batch = np.array(pad_sequences(x_train[start:end], padding='post', value=pad_index))\n",
    "            x_train_batch = np.array(pad_sequences(x_train_freq[start:end], padding='post', value=pad_index))\n",
    "            t_train_batch = np.array(t_train[start:end])[:, None]\n",
    "\n",
    "            _, train_cost = sess.run([train, cost], feed_dict={x: x_train_batch, t: t_train_batch})\n",
    "            train_costs.append(train_cost)\n",
    "        \n",
    "        # Valid\n",
    "        valid_costs = []\n",
    "        y_pred = []\n",
    "        for i in range(n_batches_valid):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, len(x_valid))\n",
    "            # x_valid_pad = np.array(pad_sequences(x_valid[start:end], padding='post', value=pad_index))\n",
    "            x_valid_pad = np.array(pad_sequences(x_valid_freq[start:end], padding='post', value=pad_index))\n",
    "            t_valid_pad = np.array(t_valid[start:end])[:, None]\n",
    "            \n",
    "            pred, valid_cost = sess.run([test, cost], feed_dict={x: x_valid_pad, t: t_valid_pad})\n",
    "            y_pred += pred.flatten().tolist()\n",
    "            valid_costs.append(valid_cost)\n",
    "        print('EPOCH: {}, Training Cost: {:.3f}, Validation Cost: {:.3f}, Validation F1: {:.3f}'.format(epoch+1, np.mean(train_costs), np.mean(valid_costs), f1_score(t_valid, y_pred, average='macro')))\n",
    "        if (f1_score(t_valid, y_pred, average='macro')>0.9):\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # Test\n",
    "    test_costs = []\n",
    "    test_y_pred = []\n",
    "    for i in range(n_batches_test):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        # x_test_pad = np.array(pad_sequences(x_test[start:end], padding = 'post', value = pad_index))\n",
    "        x_test_pad = np.array(pad_sequences(x_test_freq[start:end], padding = 'post', value = pad_index))\n",
    "        _pred = sess.run(test, feed_dict = {x:x_test_pad})\n",
    "        test_y_pred += _pred.flatten().tolist()\n",
    "        \n",
    "    ### 出力 ###\n",
    "    submission = pd.Series(test_y_pred, name='label')\n",
    "    submission.to_csv('/root/userspace/chap08/submission/submission_pred_LSTM_freq_Conv.csv', header=True, index_label='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "30000/30000 [==============================] - 212s 7ms/step - loss: 0.3917 - acc: 0.8102\n",
      "Epoch 2/3\n",
      "30000/30000 [==============================] - 210s 7ms/step - loss: 0.2383 - acc: 0.9067\n",
      "Epoch 3/3\n",
      "30000/30000 [==============================] - 211s 7ms/step - loss: 0.2017 - acc: 0.9238\n",
      "Accuracy: 89.54%\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-dd84493daf7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/userspace/chap08/submission/submission_pred_LSTM_freq_Conv.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 data = sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 262\u001b[0;31m                                       raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data must be 1-dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap08/submission/submission_code_Keras_conv.py\n",
    "\n",
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "### データの準備 ###\n",
    "top_words = 5000 # We only use most frequent 5000 words\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train)\n",
    "\n",
    "### 頻度の高い単語のみを抽出 ###\n",
    "def freq_words(x, top_words = 5000):\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        x[i] = [j for j in x[i] if j <= top_words]\n",
    "    return x\n",
    "\n",
    "### 抽出された軽いデータ ###\n",
    "x_train_freq = freq_words(x_train)\n",
    "x_valid_freq = freq_words(x_valid)\n",
    "x_test_freq = freq_words(x_test)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "x_train_pad = sequence.pad_sequences(x_train_freq, maxlen=max_review_length)\n",
    "x_valid_pad = sequence.pad_sequences(x_valid_freq, maxlen=max_review_length)\n",
    "x_test_pad = sequence.pad_sequences(x_test_freq, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train_pad, t_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_valid_pad, t_valid, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "y_pred = model.predict(x_test_pad)\n",
    "y_pred = np.round(y_pred).reshape(-1,)\n",
    "submission = pd.Series(y_pred, name='label')\n",
    "submission.to_csv('/root/userspace/chap08/submission/submission_pred_LSTM_keras.csv', header=True, index_label='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2649611 ]\n",
      " [0.05151569]\n",
      " [0.01493361]\n",
      " [0.98068684]\n",
      " [0.98333436]\n",
      " [0.0089713 ]\n",
      " [0.9959    ]\n",
      " [0.9520119 ]\n",
      " [0.02216182]]\n"
     ]
    }
   ],
   "source": [
    "# y_pred_round = round(y_pred)\n",
    "# submission = pd.Series(y_pred, name='label')\n",
    "# submission.to_csv('/root/userspace/chap08/submission/submission_pred_LSTM_freq_Conv.csv', header=True, index_label='id')\n",
    "print(y_pred[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_round = np.round(y_pred).reshape(-1,)\n",
    "submission = pd.Series(y_pred_round, name = 'label')\n",
    "submission.to_csv('/root/userspace/chap08/submission/submission_pred_LSTM_keras.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 0. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_round[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       1.0\n",
      "4       1.0\n",
      "5       0.0\n",
      "6       1.0\n",
      "7       1.0\n",
      "8       0.0\n",
      "9       1.0\n",
      "10      1.0\n",
      "11      0.0\n",
      "12      0.0\n",
      "13      1.0\n",
      "14      0.0\n",
      "15      1.0\n",
      "16      1.0\n",
      "17      0.0\n",
      "18      1.0\n",
      "19      1.0\n",
      "20      1.0\n",
      "21      1.0\n",
      "22      1.0\n",
      "23      0.0\n",
      "24      1.0\n",
      "25      1.0\n",
      "26      1.0\n",
      "27      1.0\n",
      "28      1.0\n",
      "29      0.0\n",
      "       ... \n",
      "9970    0.0\n",
      "9971    0.0\n",
      "9972    0.0\n",
      "9973    1.0\n",
      "9974    0.0\n",
      "9975    1.0\n",
      "9976    0.0\n",
      "9977    0.0\n",
      "9978    0.0\n",
      "9979    0.0\n",
      "9980    0.0\n",
      "9981    0.0\n",
      "9982    1.0\n",
      "9983    0.0\n",
      "9984    1.0\n",
      "9985    1.0\n",
      "9986    1.0\n",
      "9987    0.0\n",
      "9988    0.0\n",
      "9989    0.0\n",
      "9990    0.0\n",
      "9991    0.0\n",
      "9992    1.0\n",
      "9993    0.0\n",
      "9994    1.0\n",
      "9995    0.0\n",
      "9996    1.0\n",
      "9997    0.0\n",
      "9998    1.0\n",
      "9999    1.0\n",
      "Name: label, Length: 10000, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
