{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8回講義 演習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "課題1. 計算グラフ上での系列走査\n",
    "\n",
    "課題2. Recurrent Neural Network (RNN) によるIMDbのsentiment analysis\n",
    "1. データセットの読み込み\n",
    "2. 各層クラスの実装\n",
    "3. 計算グラフ構築 & パラメータの更新設定\n",
    "4. 学習\n",
    "\n",
    "課題3. Cellを用いたRNNの記述\n",
    "\n",
    "課題4. Long short-term memory (LSTM)\n",
    "\n",
    "【補足】Gradient Clipping（長系列への対処法）\n",
    "\n",
    "【補足】 Eager Executionについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題1. 計算グラフ上での系列走査"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflowの計算グラフ上でRNNで扱うような系列全体を走査するには、 `tf.scan`関数を使用します。\n",
    "\n",
    "系列全体にある関数を適用しながらfor文を回すことに対応します。\n",
    "\n",
    "参考:\n",
    "https://www.tensorflow.org/api_docs/python/tf/scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例:Accumulation function for vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fn(a, x):\n",
    "    return a + x\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "res = tf.scan(fn=fn, elems=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  3.  6. 10. 15. 21.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(res, feed_dict={x: np.array([1, 2, 3, 4, 5, 6])}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように`tf.scan`関数では、 2つの引数を取る関数`fn`と系列`elems`を指定します。\n",
    "\n",
    "`fn`の引数はそれぞれ役割が以下のように決まっています。\n",
    "  - 第1引数: 前ステップのfnの出力\n",
    "  - 第2引数: 今ステップの入力(elems)\n",
    "  \n",
    "つまり、`tf.scan`は以下のように`elems`の各要素に`fn`を適用します。(N：elemsの系列長)\n",
    "\n",
    "$f_0 = elems[0]$\n",
    "\n",
    "$f_1={\\rm fn}(f_0, elems[1])$\n",
    "\n",
    "$f_2={\\rm fn}(f_1, elems[2])$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$f_{N-1}={\\rm fn}(f_{N-2}, elems[N-1])$\n",
    "\n",
    "`tf.scan`の出力はこの$f_0, \\ldots, f_{N-1}$の系列です。\n",
    "\n",
    "先程の例では\n",
    "\n",
    "$f_0 = 1$\n",
    "\n",
    "$f_1={\\rm fn}(f_0, elems[1]) = 1 + 2 = 3$\n",
    "\n",
    "$f_2={\\rm fn}(f_1, elems[2]) = 3 + 3 = 6$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "となり、最終的に`tf.scan`の出力は$[1, 3, 6, 10, 15, 21]$となったわけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例:Accumulation function for matrix\n",
    "行列の場合、行が列よりも中に入っているので、行方向にプラスされる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fn(a, x):\n",
    "    return a + x\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "res = tf.scan(fn=fn, elems=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.  4.  5.]\n",
      " [ 2.  4.  6.  8. 10.]\n",
      " [ 3.  6.  9. 12. 15.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(res, feed_dict={\n",
    "            x: np.array([[1, 2, 3, 4, 5],\n",
    "                         [1, 2, 3, 4, 5],\n",
    "                         [1, 2, 3, 4, 5]])\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例: initializer\n",
    "\n",
    "`tf.scan`関数にはinitializer引数を指定することも可能です。\n",
    "\n",
    "これによりloopの初期値を明示的に指定でき、以下のように機能します。\n",
    "\n",
    "$f_0={\\rm fn}(initializer, elems[0])$\n",
    "\n",
    "$f_1={\\rm fn}(f_1, elems[1])$\n",
    "\n",
    "$f_2={\\rm fn}(f_2, elems[2])$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$f_{N-1}={\\rm fn}(f_{N-1}, elems[N-1])$\n",
    "\n",
    "なおinitializerがない場合、上記のように入力系列の最初が初期値となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fn(a, x):\n",
    "    return x[0] - x[1] + a\n",
    "\n",
    "x = (tf.placeholder(tf.float32), tf.placeholder(tf.float32))\n",
    "init = tf.placeholder(tf.float32)\n",
    "res = tf.scan(fn=fn, elems=x, initializer=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "elems = np.array([1, 2, 3, 4, 5, 6])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(res, feed_dict={\n",
    "            x: (elems+1, elems),\n",
    "            init: np.array(0)\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例: フィボナッチ数列（initializerを利用）\n",
    "$F_0 = 0,$\n",
    "$F_1 = 1,$\n",
    "$F_{n + 2} = F_n + F_{n + 1} (n ≧ 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x`は`tf.scan`を回すためだけに置かれていて、計算には全く使われていない点に注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1.,  1.,  2.,  3.,  5.,  8., 13., 21., 34., 55.], dtype=float32), array([ 1.,  2.,  3.,  5.,  8., 13., 21., 34., 55., 89.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "def fn(a, x):\n",
    "    return (a[1], a[0] + a[1])\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "init = (tf.placeholder(tf.float32), tf.placeholder(tf.float32))\n",
    "res = tf.scan(fn= fn, elems = x, initializer = init)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(res, feed_dict={init: (0,1),\n",
    "                                  x: np.zeros(10)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-125cd2aec303>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-125cd2aec303>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    res = # WRITE ME tf.scan()\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def fn(a, x):\n",
    "    return # WRITE ME\n",
    "\n",
    "res = # WRITE ME tf.scan()\n",
    "\n",
    "# fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(res, feed_dict={\n",
    "            # WRITE ME\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題2. Recurrent Neural Network (RNN) によるIMDbのsentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDb (Internet Movie Database) と呼ばれるデータセットには、映画のレビュー文とその評価がpositiveかnegativeかが記録されています。\n",
    "\n",
    "<div style=\"text-align: center;\">【データセットのイメージ】</div>\n",
    "\n",
    "| レビュー | 評価 |\n",
    "|:--------:|:-------------:|\n",
    "|Where's Michael Caine when you need him? I've ...|negative|\n",
    "|To experience Head you really need to understa...|positive|\n",
    "\n",
    "そこで各レビュー文を入力として、その評価をRNNを用いて予測してみましょう。\n",
    "\n",
    "**なおレビュー文(X)は、各単語を出現頻度順での数字に置き換えたものとして表され、評価(y)はpositiveを1、negativeを0として表しています。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データセットの読み込み\n",
    "\n",
    "`keras.datasets`内の`imdb.load_data`関数を用いてIMDbのデータセットを読み込みましょう。\n",
    "\n",
    "```python\n",
    "imdb.load_data(path=\"imdb.npz\", num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3)\n",
    "```\n",
    "引数に関して簡単に紹介しておくと、\n",
    "- **num_words**：最大インデックス（出現頻度上位`num_words-index_from`個の単語にのみIDが割り振られる）\n",
    "- skip_top：各系列の冒頭読み飛ばし長\n",
    "- maxlen：各系列の最大長（超過分は切り捨て）\n",
    "- seed：系列のシャッフル用の乱数シード値\n",
    "- **start_char**：系列開始（BOS）記号用インデックス\n",
    "- **oov_char**：その他用（`skip_top`部分や`num_words`超過分）インデックス\n",
    "- **index_from**：単語IDの開始インデックス\n",
    "\n",
    "なお、0は通常後述するpaddingに用いられるため、意図的に回避したナンバリングがデフォルトとして設定されています。\n",
    "\n",
    "読み込みの細かい設定については公式ドキュメントを参照してください。\n",
    "\n",
    "参考：https://keras.io/ja/datasets/#imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "pad_index = 0\n",
    "num_words = 10000\n",
    "(x_train, t_train), (x_test, t_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# データセットサイズが大きいので演習用に短縮\n",
    "x_train = x_train[:len(x_train)//2]\n",
    "t_train = t_train[:len(t_train)//2]\n",
    "x_valid = x_valid[:len(x_valid)//2]\n",
    "t_valid = t_valid[:len(t_valid)//2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imdbの観測値は、レビューの単語に(対応する辞書の)インデックスを割り当てたものと考えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 73, 89, 81, 25, 60, 967, 6, 20, 141, 17, 14, 31, 127, 12, 60, 28, 1360, 1107, 66, 45, 6, 20, 15, 497, 8, 79, 17, 491, 8, 112, 6, 6683, 20, 17, 614, 691, 4, 436, 20, 9, 2855, 6, 762, 7, 493, 8621, 6, 185, 250, 24, 55, 2276, 5, 23, 350, 7, 15, 82, 24, 15, 821, 66, 10, 10, 45, 578, 15, 4, 20, 805, 8, 30, 17, 821, 5, 1621, 17, 614, 190, 4, 20, 9, 43, 32, 99, 1214, 18, 15, 8, 157, 46, 17, 1436, 4, 2, 5, 2, 9, 32, 1796, 5, 1214, 267, 17, 73, 17, 4413, 36, 26, 400, 43, 4562, 83, 4, 1873, 247, 74, 83, 4, 250, 540, 82, 4, 96, 4, 250, 8306, 8, 32, 4, 2, 9, 184, 3966, 13, 384, 48, 14, 16, 147, 1348, 59, 62, 69, 9420, 12, 46, 50, 9, 53, 2, 74, 1930, 11, 14, 31, 151, 10, 10, 4, 20, 9, 540, 364, 352, 5, 45, 6, 2, 589, 33, 269, 8, 2715, 142, 1621, 5, 821, 17, 73, 17, 204, 5, 2908, 19, 55, 1763, 4697, 92, 66, 104, 14, 20, 93, 76, 1488, 151, 33, 4, 58, 12, 188, 626, 151, 12, 215, 69, 224, 142, 73, 237, 6, 964, 7, 1446, 2289, 188, 626, 103, 14, 31, 10, 10, 451, 7, 1465, 5, 599, 80, 91, 1329, 30, 685, 34, 14, 20, 151, 50, 26, 131, 49, 7717, 84, 46, 50, 37, 80, 79, 6, 1968, 46, 7, 14, 20, 10, 10, 470, 158]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDbの各レビューは長さが異なります。したがって、可変長の系列に対してRNNを適用し、最後の隠れ状態を元に二値分類を行うということになります。\n",
    "\n",
    "ですが、RNNの各バッチでの入力は同じ長さでないと、行列として計算が行えなくなってしまいます。\n",
    "\n",
    "そこで実際にはまず、\n",
    "- ミニバッチ内のデータの系列長を揃える（**padding**）\n",
    "\n",
    "必要があります。\n",
    "\n",
    "つまり系列の先頭あるいは末尾に、系列長の調整用であることを表す値（今回は0）を付加し、バッチ内の系列長を統一します。\n",
    "\n",
    "これは `keras.preprocessing.sequence` にある関数 `pad_sequences` を使うことなどで可能です。\n",
    "\n",
    "（ミニバッチごとの対応になるので、後ほど 4. 学習にて利用します）\n",
    "\n",
    "参考：https://keras.io/ja/preprocessing/sequence/#pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x_train_batch = np.array(pad_sequences(x_train[start:end], padding='post', value=pad_index)) # バッチ毎のPadding\n",
    "t_train_batch = np.array(t_train[start:end])[:, None]\n",
    "\n",
    "_, train_cost = sess.run([train, cost], feed_dict={x: x_train_batch, t: t_train_batch})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意事項(感動)\n",
    "またpaddingが多くなると計算的に非効率になるため、paddingを少なくする目的で予めデータの長さで降順にソートしておくことが多いです。  \n",
    "ちなみに、`[len(com) for com in x_train]`はリスト内包表記と呼ばれるもので、意味的には以下と同じ。\n",
    "```python\n",
    "a = []\n",
    "for com in x_train:\n",
    "    a = a.append(len(com))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainデータを長さ順にソート\n",
    "x_train_lens = [len(com) for com in x_train]  #各行のLengthのベクトル\n",
    "sorted_train_indexes = sorted(range(len(x_train_lens)), key=lambda x: -x_train_lens[x]) #sortedは昇順に並べてしまうのでkeyが-x_train_lensになっているのか\n",
    "\n",
    "x_train = [x_train[ind] for ind in sorted_train_indexes]\n",
    "t_train = [t_train[ind] for ind in sorted_train_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6161"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_train_indexes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、paddingの部分はあくまで系列長を合わせるためなので、通常のRNNの計算はおこなわず、何らかの形で計算を無効にする必要があります。\n",
    "\n",
    "その具体的な方法は後ほどRNN層の実装時に詳しく扱います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 各層クラスの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Embedding層\n",
    "\n",
    "Embedding層では、単語を離散的なidから連続的な数百次元のベクトルに変換(埋め込み、embed)します。\n",
    "\n",
    "下のEmbeddingクラスにおいて、入力$\\boldsymbol{x}$は各行に文の単語のid列が入った行列で、重み$\\boldsymbol{V}$は各行がそれぞれの単語idのベクトルに対応した行列です。\n",
    "\n",
    "つまりそれぞれの行列のサイズは\n",
    "\n",
    "- $\\boldsymbol{x}$: (ミニバッチサイズ) x (ミニバッチ内の文の最大系列長)\n",
    "- $\\boldsymbol{V}$: (辞書の単語数) x (単語のベクトルの次元数)\n",
    "\n",
    "です。\n",
    "\n",
    "この$\\boldsymbol{V}$から、入力$\\boldsymbol{x}$のそれぞれの単語idに対して対応する単語ベクトルを取り出すことで、各単語をベクトルに変換します。\n",
    "\n",
    "`tf`では`tf.nn.embedding_lookup`によりこの作業を行います。\n",
    "\n",
    "この処理によって出力されるテンソルの次元数は、(ミニバッチサイズ) x (ミニバッチ内の文の最大系列長) x (単語のベクトルの次元数)となります。\n",
    "\n",
    "![embedding](../figures/embedding.png)\n",
    "\n",
    "$$m:\\text{emb_dim}, \\ n : \\text{vocab_size}$$\n",
    "\n",
    "参考：https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, emb_dim, scale=0.08):\n",
    "        self.V = tf.Variable(tf.random_normal(shape = (vocab_size, emb_dim), stddev = scale), name = \"V\")\n",
    "        # WRITE ME\n",
    " \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.embedding_lookup(self.V, x)\n",
    "        # WRITE ME tf.nn.embedding_lookup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. RNN\n",
    "\n",
    "RNNクラスでは、Embedding層で各単語がベクトルに変換されたものを入力として処理を行います。ここで入力$\\boldsymbol{x}$は\n",
    "\n",
    "- $\\boldsymbol{x}$: (ミニバッチサイズ) x (ミニバッチ内の文の最大系列長) x (単語のベクトルの次元数)\n",
    "\n",
    "となっています。`tf.scan`では第0軸方向に走査していくので、文の系列方向に沿って走査するために上の第0軸と第1軸を`tf.transpose`により入れ替えて\n",
    "\n",
    "- $\\boldsymbol{x}$: (ミニバッチ内の文の最大系列長) x (ミニバッチサイズ) x (単語のベクトルの次元数)\n",
    "\n",
    "とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また先述の通り、**paddingの部分の計算を無効化**する必要があります。\n",
    "\n",
    "ここではわかりやすい実装として、padding部では隠れ状態を変更しない、つまり前のステップの隠れ状態をそのままコピーすることにしましょう。\n",
    "\n",
    "こうすることで、実際の系列の末尾における隠れ状態を保持するようにします。\n",
    "\n",
    "実装としては、各系列の実際の系列長を表す`seq_len`を元に、実際に単語がある部分に1、padding部に0を置くバイナリマスク(多分、$T:=$`seq_len`)\n",
    "\n",
    "$$\\boldsymbol{m}=[m_1, m_2, \\dots, m_t, \\dots, m_T]$$\n",
    "\n",
    "を`tf.sequence_mask`によって作成し、隠れ状態の更新時に以下のようにして適用します。（単純化のため文末にのみpaddingがあると想定します。）\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{h}_t = m_t \\cdot \\sigma\\left(\\boldsymbol{W}\\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}\\right) + (1-m_t) \\cdot \\boldsymbol{h}_{t-1}\n",
    "$$\n",
    "\n",
    "こうすることでpaddingの部分では$\\boldsymbol{h}_t=\\boldsymbol{h}_{t-1}$となり、paddingの計算結果に対する影響がなくなります。\n",
    "\n",
    "参考：https://www.tensorflow.org/api_docs/python/tf/sequence_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, in_dim, hid_dim, seq_len=None, scale=0.08):\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        # Initializationは今までと同様に行います。\n",
    "        glorot = tf.cast(tf.sqrt(6/(in_dim + hid_dim*2)), tf.float32)\n",
    "        self.W = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval = -glorot, maxval = glorot), name = \"W\")\n",
    "        # WRITE ME\n",
    "        self.b = tf.Variable(tf.zeros([hid_dim]), name = \"b\")\n",
    "        # WRITE ME\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.initial_state = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # tf.scanへの適用関数fn\n",
    "        # WRITE ME\n",
    "        def fn(h_prev, x_and_m): # 次にfnを使う際に、h_prevに計算結果が入ると考えよ\n",
    "            x_t, m_t = x_and_m\n",
    "            inputs = tf.concat([x_t, h_prev], -1) # -1の意味について答えられるようにしよう\n",
    "            # RNNの適用\n",
    "            h_t = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)\n",
    "            # Maskの適用\n",
    "            h_t = m_t*h_t + (1 - m_t)*h_prev\n",
    "            return h_t\n",
    "        \n",
    "\n",
    "        # 入力の整形\n",
    "        # WRITE ME\n",
    "        # 入力の時間を並べる\n",
    "        x_tmaj = tf.transpose(x, perm = [1, 0, 2])\n",
    "        \n",
    "        # マスクの生成\n",
    "        # WRITE ME\n",
    "        # tf.cast(x, dtype)でxのdtypeを指定したタイプに変換する。\n",
    "        # tf.sequence_mask(lengths, max_len)で与える\n",
    "        # lengthsで与えた長さだけTrueの、max_lenで指定した長さのベクトルを返す\n",
    "        # tf.expand_dimsはaxisで指定した場所に1次元(a dimension of 1)追加する。\n",
    "        mask = tf.cast(tf.sequence_mask(self.seq_len, tf.shape(x)[1]), tf.float32)\n",
    "        mask_tmaj = tf.transpose(tf.expand_dims(mask, axis = -1), perm = [1, 0, 2])\n",
    "        \n",
    "        if self.initial_state is None:\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            self.initial_state = tf.zeros([batch_size, self.hid_dim])\n",
    "            # WRITE ME\n",
    "        \n",
    "        h = tf.scan(fn = fn, elems = [x_tmaj, mask_tmaj], initializer = self.initial_state)\n",
    "        # WRITE ME tf.scan()\n",
    "        \n",
    "        return h[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 計算グラフ構築 & パラメータの更新設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.not_equal([1,2], [3,2])\n",
    "np.sum(np.not_equal([1,2], [3,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # グラフ初期化\n",
    "\n",
    "emb_dim = 100\n",
    "hid_dim = 50\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "t = tf.placeholder(tf.float32, [None, None], name='t')\n",
    "\n",
    "\n",
    "# tf.not_equal(x,y)は x!=yの真偽値を返す。\n",
    "# pad_indexはpadding用に予約されたindexの値で、pad_index = 0\n",
    "\n",
    "seq_len = tf.reduce_sum(tf.cast(tf.not_equal(x, pad_index), tf.int32), axis=1)\n",
    "# WRITE ME\n",
    "\n",
    "h = Embedding(num_words, emb_dim)(x)\n",
    "h = RNN(emb_dim, hid_dim, seq_len)(h)\n",
    "y = tf.layers.Dense(1, tf.nn.sigmoid)(h)\n",
    "\n",
    "cost = -tf.reduce_mean(t*tf_log(y) + (1 - t)*tf_log(1 - y))\n",
    "\n",
    "train = tf.train.AdamOptimizer().minimize(cost)\n",
    "test = tf.round(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 学習\n",
    "\n",
    "バッチの入力に注目しつつ、学習ループを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Training Cost: 0.624, Validation Cost: 0.581, Validation F1: 0.682\n",
      "EPOCH: 2, Training Cost: 0.432, Validation Cost: 0.469, Validation F1: 0.793\n",
      "EPOCH: 3, Training Cost: 0.285, Validation Cost: 0.512, Validation F1: 0.793\n",
      "EPOCH: 4, Training Cost: 0.204, Validation Cost: 0.553, Validation F1: 0.786\n",
      "EPOCH: 5, Training Cost: 0.232, Validation Cost: 0.654, Validation F1: 0.757\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 100\n",
    "n_batches_train = len(x_train) // batch_size\n",
    "n_batches_valid = len(x_valid) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_costs = []\n",
    "        for i in range(n_batches_train):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            x_train_batch = np.array(pad_sequences(x_train[start:end], padding='post', value=pad_index)) # バッチ毎のPadding\n",
    "            t_train_batch = np.array(t_train[start:end])[:, None]\n",
    "\n",
    "            _, train_cost = sess.run([train, cost], feed_dict={x: x_train_batch, t: t_train_batch})\n",
    "            train_costs.append(train_cost)\n",
    "        \n",
    "        # Valid\n",
    "        valid_costs = []\n",
    "        y_pred = []\n",
    "        for i in range(n_batches_valid):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            x_valid_pad = np.array(pad_sequences(x_valid[start:end], padding='post', value=pad_index)) # バッチ毎のPadding\n",
    "            t_valid_pad = np.array(t_valid[start:end])[:, None]\n",
    "            \n",
    "            pred, valid_cost = sess.run([test, cost], feed_dict={x: x_valid_pad, t: t_valid_pad})\n",
    "            y_pred += pred.flatten().tolist()\n",
    "            valid_costs.append(valid_cost)\n",
    "        print('EPOCH: %i, Training Cost: %.3f, Validation Cost: %.3f, Validation F1: %.3f' % (epoch+1, np.mean(train_costs), np.mean(valid_costs), f1_score(t_valid, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題3. Cellを用いたRNNの記述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題2. までは原理の理解のため、loopを構成する方法としてscanを紹介・利用の上、RNNを実装しました。\n",
    "\n",
    "ここからはより実践的な実装として、RNNの各時点の処理に対応する**Cell構造**を用いたRNNの実装を扱います。\n",
    "\n",
    "この方法では明示的にloopを構成することなくRNNを実装できます。\n",
    "\n",
    "具体的には、`tf.nn.rnn_cell.BasicRNNCell()`を用いてCell構造を生成した後、`tf.nn.static_rnn`でCellに基づいたRNNを構成します。\n",
    "\n",
    "参考：\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hid_dim, initial_state, maxlen):\n",
    "        self.cell = tf.nn.rnn_cell.BasicRNNCell(hid_dim) # RNNのCell構造の生成 引数は num_units = hid_dim\n",
    "        self.initial_state = initial_state\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # tf.unstack(x, axis, num)はxを指定したaxisでsliceしてテンソルの次元を落とす\n",
    "        # numはaxisで指定した軸での長さ。Noneなら自動的に推定される\n",
    "        # ここでの1-axisはミニバッチ内の文の最大系列長という事になるか?\n",
    "        # inputsは、テンソルのリストになっており、１つのテンソルは(バッチサイズ)*(単語ベクトルの次元数)\n",
    "        inputs = tf.unstack(x, num=self.maxlen, axis=1) \n",
    "        outputs, state = tf.nn.static_rnn(self.cell, inputs, self.initial_state)\n",
    "        return outputs[-1]\n",
    "    \n",
    "    # あるいは以下のようにも書ける\n",
    "    #def __call__(self, x):\n",
    "    #    outputs = []\n",
    "    #    state = self.initial_state\n",
    "    #    with tf.variable_scope(\"RNN\"):\n",
    "    #        for t in range(self.maxlen):\n",
    "    #            if t > 0:\n",
    "    #                tf.get_variable_scope().reuse_variables()\n",
    "    #            (cell_output, state) = self.cell(x[:, t, :], state)\n",
    "    #            outputs.append(cell_output)\n",
    "    #    return outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、このような処理では**可変長の系列を適切に扱うことができません**。\n",
    "\n",
    "具体的には\n",
    "\n",
    "- maxlenを明示的かつ固定長で与える必要があり、ミニバッチ毎に異なるmaxlenへ対応できない\n",
    "- バッチ内でも系列ごとに長さは異なり、マスキングが必要\n",
    "\n",
    "となってしまっています。\n",
    "\n",
    "2点目のマスキングについては、実は`tf.nn.static_rnn`に引数`sequence_length`としてバッチ内の各系列長を与えれば解決が可能です。\n",
    "\n",
    "1点目については、`tf.nn.static_rnn`に代えて`tf.nn.dynamic_rnn`という関数を用いることで、ミニバッチ毎にmaxlenが異なっても対応可能になります。\n",
    "\n",
    "またこちらも引数`sequence_length`を受け取るため、2つの難点を同時に解決できます。よって以降基本的に`tf.nn.dynamic_rnn`を用いることとします。\n",
    "\n",
    "（なお後述するeager modeを使用しても解決が可能です）\n",
    "\n",
    "`static_rnn`ではテンソルの**リスト**（長さ：maxlen）で`input`を受け取っていましたが、`dynamic_rnn`では`[batch_size, maxlen, emb_dim]`のテンソルで受け取っているので注意してください。\n",
    "\n",
    "参考：https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hid_dim, seq_len = None, initial_state = None):\n",
    "        self.cell = tf.nn.rnn_cell.BasicRNNCell(hid_dim)\n",
    "        self.initial_state = initial_state\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.initial_state is None:\n",
    "            self.initial_state = self.cell.zero_state(tf.shape(x)[0], tf.float32)\n",
    "            \n",
    "        # outputsは各系列長分以降は0になるので注意\n",
    "        outputs, state = tf.nn.dynamic_rnn(self.cell, x, self.seq_len, self.initial_state)\n",
    "        return tf.gather_nd(outputs, indices = tf.stack([tf.range(tf.shape(x)[0]), self.seq_len-1], axis = 1 ))\n",
    "        # WRITE ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-48ca1ebedbed>:3: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # グラフ初期化\n",
    "\n",
    "emb_dim = 100\n",
    "hid_dim = 50\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "t = tf.placeholder(tf.float32, [None, None], name='t')\n",
    "\n",
    "seq_len = tf.reduce_sum(tf.cast(tf.not_equal(x, pad_index), tf.int32), axis=1)\n",
    "\n",
    "h = Embedding(num_words, emb_dim)(x)\n",
    "h = RNN(hid_dim, seq_len)(h)\n",
    "y = tf.layers.Dense(1, tf.nn.sigmoid)(h)\n",
    "\n",
    "cost = -tf.reduce_mean(t*tf_log(y) + (1 - t)*tf_log(1 - y))\n",
    "\n",
    "train = tf.train.AdamOptimizer().minimize(cost)\n",
    "test = tf.round(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Training Cost: 0.638, Validation Cost: 0.541, Validation F1: 0.730\n",
      "EPOCH: 2, Training Cost: 0.559, Validation Cost: 0.487, Validation F1: 0.775\n",
      "EPOCH: 3, Training Cost: 0.355, Validation Cost: 0.464, Validation F1: 0.790\n",
      "EPOCH: 4, Training Cost: 0.432, Validation Cost: 0.587, Validation F1: 0.692\n",
      "EPOCH: 5, Training Cost: 0.227, Validation Cost: 0.562, Validation F1: 0.768\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 100\n",
    "n_batches_train = len(x_train) // batch_size\n",
    "n_batches_valid = len(x_valid) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_costs = []\n",
    "        for i in range(n_batches_train):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            x_train_batch = np.array(pad_sequences(x_train[start:end], padding='post', value=pad_index))\n",
    "            t_train_batch = np.array(t_train[start:end])[:, None]\n",
    "\n",
    "            _, train_cost = sess.run([train, cost], feed_dict={x: x_train_batch, t: t_train_batch})\n",
    "            train_costs.append(train_cost)\n",
    "        \n",
    "        # Valid\n",
    "        valid_costs = []\n",
    "        y_pred = []\n",
    "        for i in range(n_batches_valid):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            x_valid_pad = np.array(pad_sequences(x_valid[start:end], padding='post', value=pad_index))\n",
    "            t_valid_pad = np.array(t_valid[start:end])[:, None]\n",
    "            \n",
    "            pred, valid_cost = sess.run([test, cost], feed_dict={x: x_valid_pad, t: t_valid_pad})\n",
    "            y_pred += pred.flatten().tolist()\n",
    "            valid_costs.append(valid_cost)\n",
    "        print('EPOCH: %i, Training Cost: %.3f, Validation Cost: %.3f, Validation F1: %.3f' % (epoch+1, np.mean(train_costs), np.mean(valid_costs), f1_score(t_valid, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題4. Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装する式は次のようになります。($\\odot$は要素ごとの積)\n",
    "\n",
    "- 入力ゲート: $\\hspace{20mm}\\boldsymbol{i}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_i \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_i\\right)$\n",
    "- 忘却ゲート: $\\hspace{20mm}\\boldsymbol{f}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_f \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_f\\right)$  \n",
    "- 出力ゲート: $\\hspace{20mm}\\boldsymbol{o}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_o \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_o\\right)$  \n",
    "- セル:　　　 $\\hspace{20mm}\\boldsymbol{c}_t = \\boldsymbol{f}_t \\odot \\boldsymbol{c}_{t-1} + \\boldsymbol{i}_t \\odot \\tanh \\left(\\boldsymbol{W}_c \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_c\\right)$\n",
    "- 隠れ状態: 　$\\hspace{20mm}\\boldsymbol{h}_t = \\boldsymbol{o}_t \\odot \\tanh \\left(\\boldsymbol{c}_t \\right)$\n",
    "\n",
    "単純なRNNでは各ステップの関数の戻り値は隠れ状態のみ ($\\boldsymbol{h}_t$) でしたが、LSTMではセル状態と隠れ状態の2つ ($\\boldsymbol{c}_t, \\boldsymbol{h}_t$) となるので注意してください。\n",
    "\n",
    "またマスクに関しても両方に適用する必要があります。\n",
    "\n",
    "まずは、愚直に`tf.scan`を用いて実装してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マスクの適用の式\n",
    "$$\n",
    "c_t = m_t \\times c_t + (1-m_t) \\times c_{t-1}\\\\\n",
    "h_t = m_t \\times h_t + (1-m_t) \\times h_{t-1}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, in_dim, hid_dim, seq_len = None, initial_state = None):\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        glorot = tf.cast(tf.sqrt(6/(in_dim + hid_dim*2)), tf.float32)\n",
    "        \n",
    "        # 入力ゲート\n",
    "        self.W_i = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_i')\n",
    "        self.b_i  = tf.Variable(tf.zeros([hid_dim]), name='b_i')\n",
    "        \n",
    "        # 忘却ゲート\n",
    "        self.W_f = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_f')\n",
    "        self.b_f  = tf.Variable(tf.zeros([hid_dim]), name='b_f')\n",
    "\n",
    "        # 出力ゲート\n",
    "        self.W_o = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_o')\n",
    "        self.b_o  = tf.Variable(tf.zeros([hid_dim]), name='b_o')\n",
    "\n",
    "        # セル\n",
    "        self.W_c = tf.Variable(tf.random_uniform([in_dim + hid_dim, hid_dim], minval=-glorot, maxval=glorot), name='W_c')\n",
    "        self.b_c  = tf.Variable(tf.zeros([hid_dim]), name='b_c')\n",
    "\n",
    "        # マスク\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.initial_state = initial_state\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # tf.scanへの適用関数fn\n",
    "        # WRITE ME\n",
    "        def fn(prev_state, x_and_m):\n",
    "            x_t, m_t = x_and_m\n",
    "            c_prev, h_prev = prev_state[0], prev_state[1]\n",
    "            inputs = tf.concat([x_t, h_prev], axis = -1)\n",
    "            \n",
    "            # 入力ゲート\n",
    "            i_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_i) + self.b_i)\n",
    "            # 忘却ゲート\n",
    "            f_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_f) + self.b_f)\n",
    "            # 出力ゲート\n",
    "            o_t = tf.nn.sigmoid(tf.matmul(inputs, self.W_o) + self.b_o)\n",
    "            \n",
    "            # セル\n",
    "            c_t = tf.multiply(f_t, c_prev) + tf.nn.tanh(tf.matmul(inputs, self.W_c) + self.b_c)\n",
    "            # 隠れ層\n",
    "            h_t = tf.multiply(o_t, tf.nn.tanh(c_t))\n",
    "            \n",
    "            # マスクの適用\n",
    "            c_t = m_t*c_t + (1 - m_t)*c_prev\n",
    "            h_t = m_t*h_t + (1 - m_t)*h_prev\n",
    "            # 出力\n",
    "            return tf.stack([c_t, h_t])\n",
    "\n",
    "        # 入力の時間順化\n",
    "        x_tmaj = tf.transpose(x, perm=[1, 0, 2])\n",
    "        \n",
    "        # マスクの生成＆時間順化\n",
    "        mask = tf.cast(tf.sequence_mask(self.seq_len, tf.shape(x)[1]), tf.float32)\n",
    "        mask_tmaj = tf.transpose(tf.expand_dims(mask, axis=-1), perm=[1, 0, 2])\n",
    "        \n",
    "        if self.initial_state is None:\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            self.initial_state = tf.stack([tf.zeros([batch_size, self.hid_dim]), tf.zeros([batch_size, self.hid_dim])])\n",
    "\n",
    "        state_seq = tf.scan(fn=fn, elems=[x_tmaj, mask_tmaj], initializer=self.initial_state)\n",
    "        \n",
    "        return state_seq[-1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNとLSTMで同じタスクで学習させてみて、比較してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # グラフ初期化\n",
    "\n",
    "emb_dim = 100\n",
    "hid_dim = 50\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "t = tf.placeholder(tf.float32, [None, None], name='t')\n",
    "\n",
    "seq_len = tf.reduce_sum(tf.cast(tf.not_equal(x, pad_index), tf.int32), axis=1)\n",
    "\n",
    "h = Embedding(num_words, emb_dim)(x)\n",
    "h = LSTM(emb_dim, hid_dim, seq_len)(h)\n",
    "y = tf.layers.Dense(1, tf.nn.sigmoid)(h)\n",
    "\n",
    "cost = -tf.reduce_mean(t*tf_log(y) + (1 - t)*tf_log(1 - y))\n",
    "\n",
    "train = tf.train.AdamOptimizer().minimize(cost)\n",
    "test = tf.round(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Training Cost: 0.617, Validation Cost: 0.529, Validation F1: 0.742\n",
      "EPOCH: 2, Training Cost: 0.391, Validation Cost: 0.462, Validation F1: 0.788\n",
      "EPOCH: 3, Training Cost: 0.405, Validation Cost: 0.514, Validation F1: 0.771\n",
      "EPOCH: 4, Training Cost: 0.387, Validation Cost: 0.524, Validation F1: 0.741\n",
      "EPOCH: 5, Training Cost: 0.276, Validation Cost: 0.504, Validation F1: 0.788\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 100\n",
    "n_batches_train = len(x_train) // batch_size\n",
    "n_batches_valid = len(x_valid) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_costs = []\n",
    "        for i in range(n_batches_train):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            x_train_batch = np.array(pad_sequences(x_train[start:end], padding='post', value=pad_index))\n",
    "            t_train_batch = np.array(t_train[start:end])[:, None]\n",
    "\n",
    "            _, train_cost = sess.run([train, cost], feed_dict={x: x_train_batch, t: t_train_batch})\n",
    "            train_costs.append(train_cost)\n",
    "        \n",
    "        # Valid\n",
    "        valid_costs = []\n",
    "        y_pred = []\n",
    "        for i in range(n_batches_valid):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            x_valid_pad = np.array(pad_sequences(x_valid[start:end], padding='post', value=pad_index))\n",
    "            t_valid_pad = np.array(t_valid[start:end])[:, None]\n",
    "            \n",
    "            pred, valid_cost = sess.run([test, cost], feed_dict={x: x_valid_pad, t: t_valid_pad})\n",
    "            y_pred += pred.flatten().tolist()\n",
    "            valid_costs.append(valid_cost)\n",
    "        print('EPOCH: %i, Training Cost: %.3f, Validation Cost: %.3f, Validation F1: %.3f' % (epoch+1, np.mean(train_costs), np.mean(valid_costs), f1_score(t_valid, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMについても、以下のようにしてcellを用いることができます。`tf.nn.rnn_cell.BasicLSTMCell`を使用しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, hid_dim, seq_len = None, initial_state = None):\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell(hid_dim) # ここが新しい\n",
    "        self.initial_state = initial_state\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.initial_state is None:\n",
    "            self.initial_state = self.cell.zero_state(tf.shape(x)[0], tf.float32)\n",
    "            \n",
    "        outputs, state = tf.nn.dynamic_rnn(self.cell, x, self.seq_len, self.initial_state)\n",
    "        return tf.gather_nd(outputs, tf.stack([tf.range(tf.shape(x)[0]), self.seq_len-1], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【補足】Gradient Clipping（長系列への対処法）\n",
    "\n",
    "LSTMは長系列に対しても学習がうまく行きやすいモデルでしたが、一般のRNNにおける長系列の学習のTipsとして、**Gradient Clipping**に触れておきます。\n",
    "\n",
    "RNNでは誤差逆伝播法が特に**Back Propagation Through Time (BPTT)**と呼ばれるものになり、各層のみならず各時点の勾配が乗算されます。\n",
    "\n",
    "そのため、通常よりも勾配が過大（或いは過小）になりやすいという特徴をもっています。\n",
    "\n",
    "こうした現象を**勾配爆発（消失）**と呼びますが、勾配爆発は学習を不安定化し収束を困難にします。\n",
    "\n",
    "![Clipping](../figures/Clipping.png)\n",
    "出典：Ian Goodfellow et. al, “Deep Learning”, MIT press, 2016 (http://www.deeplearningbook.org/)\n",
    "\n",
    "そこで、勾配の大きさを意図的に制限して対処しようというのが、Gradient Clippingと呼ばれる手法です。\n",
    "\n",
    "以下のように、明示的にoptimizerから勾配を取得した後、`tf.clip_by_value`関数に通した上で勾配を適用することで実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = tf.train.AdamOptimizer().minimize(cost) を以下に置き換え\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "grads = optimizer.compute_gradients(cost)\n",
    "clipped_grads = [(tf.clip_by_value(grad_val, -1., 1.), var) for grad_val, var in grads]\n",
    "train = optimizer.apply_gradients(clipped_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【補足】 Eager Executionについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "近年、Define-and-RunであったTensorFlowに、Define-by-Runを可能にするEager Executionが導入されました。\n",
    "\n",
    "Eager Executionを用いることでTensorFlowでも動的な計算グラフの構築が実現され、例えばミニバッチ毎に処理を変えるといったことが可能になりました。\n",
    "\n",
    "もちろん、Define-by-Runには動的である故のデメリット（ex. 最適化が困難など）もあります。\n",
    "\n",
    "しかし、上述のメリットにより、特にRNN系のモデルの記述には重宝されることが多いため、今回補足事項として簡単に取り扱っておきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "# eager executionでレイヤーを定義するには、tf.keras.layers.Layerを継承する必要があります\n",
    "class EagerEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, emb_dim, scale=0.08):\n",
    "        super(EagerEmbedding, self).__init__()\n",
    "        \n",
    "        # self.add_variableでvariableの追加を行います\n",
    "        self.V = self.add_variable(\"V\", [vocab_size, emb_dim], initializer='RandomNormal')\n",
    "\n",
    "    # call関数に順伝播を実装します\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.V, inputs)\n",
    "\n",
    "class EagerRNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(EagerRNN, self).__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "    \n",
    "    # build関数でもvariableの追加が可能です（特にinput_shapeに依存したvariable）\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_variable(\"W\", [input_shape[-1] + self.hid_dim, self.hid_dim], initializer='Orthogonal')\n",
    "        self.b = self.add_variable(\"b\", [self.hid_dim], initializer='Zeros')\n",
    "\n",
    "    # ここではマスクを考慮せずに書いています、意欲的な方はぜひマスクを考慮したrnnに書き換えてみてください\n",
    "    def call(self, inputs):\n",
    "        outputs = []\n",
    "        state = tf.zeros(shape=(inputs.shape[0], self.hid_dim))\n",
    "        for t in range(inputs.shape[1]):\n",
    "            state = tf.nn.tanh(tf.matmul(tf.concat([inputs[:,t,:], state], axis=1), self.W) + self.b)\n",
    "            output = state\n",
    "            outputs.append(output)\n",
    "        return outputs[-1]\n",
    "\n",
    "# なお、RNNをcellを用いてeager executionで実装することも可能です（こちらについてもぜひ挑戦してみてください）\n",
    "\n",
    "# モデルを一つにまとめるには、tf.keras.Modelを継承したクラスを用います（あるいはtf.keras.Sequential）\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, scale=0.08):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.word_embedding = EagerEmbedding(vocab_size, emb_dim)\n",
    "        self.rnn = EagerRNN(hid_dim)\n",
    "        self.dense = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        h = self.word_embedding(inputs)\n",
    "        h = self.rnn(h)\n",
    "        y = self.dense(h)\n",
    "        return tf.reshape(y, shape=[-1])\n",
    "    \n",
    "# loss関数\n",
    "def loss_with_logits(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.cast(labels, tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eager executionでの実行を宣言(tensorflowの関数実行の最小に行う必要がある、要kernel restart)\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pad_index = 0\n",
    "num_words = 10000\n",
    "(x_train, t_train), (x_test, t_test) = imdb.load_data(num_words=num_words)\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train[:len(x_train)//2]\n",
    "t_train = t_train[:len(t_train)//2]\n",
    "x_valid = x_valid[:len(x_valid)//2]\n",
    "t_valid = t_valid[:len(t_valid)//2]\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', value=pad_index)\n",
    "x_valid = pad_sequences(x_valid, padding='post', value=pad_index)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, t_train))\n",
    "train_dataset = train_dataset.shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "hid_dim = 50\n",
    "model = Model(num_words, emb_dim, hid_dim)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "epoch_num = 10\n",
    "\n",
    "# eager executionはデフォルトでcpu実行なので、gpuを指定\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    for epoch in range(epoch_num):\n",
    "        tf.train.get_or_create_global_step()\n",
    "\n",
    "        for (i, (inputs, labels)) in enumerate(train_dataset):\n",
    "            # eager executionではminimize関数にはlossを関数として渡す必要があります（引数無しlambda）\n",
    "            optimizer.minimize(lambda: loss_with_logits(model(inputs), labels), global_step=tf.train.get_global_step())\n",
    "\n",
    "        valid_logits = model(x_valid)\n",
    "        valid_loss = loss_with_logits(valid_logits, t_valid)\n",
    "        valid_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.nn.sigmoid(valid_logits)), t_valid), tf.float32))\n",
    "\n",
    "        print(('EPOCH %02d\\t Validation Loss: %.2f\\t Validation Accuracy: %.2f') % (epoch + 1, valid_loss, valid_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装において、kerasのクラスを継承していたことからも想像できるように、kerasの各種の便利な機能を用いるにあたっても比較的相性が良くなっています。\n",
    "\n",
    "たとえば、上記の学習ループはkerasの`Model.fit()`などに置き換えることが可能です。興味のある方は調べてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
