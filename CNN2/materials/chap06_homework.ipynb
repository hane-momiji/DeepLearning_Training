{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    "\n",
    "CNNを用いて、FashionMNISTの高精度な分類器を実装してみましょう。\n",
    "\n",
    "モデルのレイヤーを変更してみるなどして精度の向上にチャンレンジして下さい。 精度上位者はリーダーボードに載ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目標値\n",
    "\n",
    "Accuracy 93%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ルール\n",
    "\n",
    "- 訓練データはx_train、 t_train、テストデータはx_testで与えられます。\n",
    "- 予測ラベルは one_hot表現ではなく0~9のクラスラベル で表してください。\n",
    "- **下のセルで指定されているx_train、t_train以外の学習データは使わないでください。**\n",
    "- Tensorflowを利用して構いません。\n",
    "- ただし、**tf.layersのような高レベルのAPIは使用しないで下さい。**具体的に以下のモジュールは使用しないでください。\n",
    "\n",
    "```\n",
    "tf.app,\n",
    "tf.compat,\n",
    "tf.contrib,\n",
    "tf.estimator,\n",
    "tf.gfile,\n",
    "tf.graph_util,\n",
    "tf.image,\n",
    "tf.initializers,\n",
    "tf.keras,\n",
    "tf.layers,\n",
    "tf.logging,\n",
    "tf.losses,\n",
    "tf.metrics,\n",
    "tf.python_io,\n",
    "tf.resource_loader,\n",
    "tf.saved_model,\n",
    "tf.sets,\n",
    "tf.summary,\n",
    "tf.sysconfig,\n",
    "tf.test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提出方法\n",
    "\n",
    "- 2つのファイルを提出していただきます。\n",
    "  - テストデータ (x_test) に対する予測ラベルをcsvファイル (ファイル名: submission_pred.csv) で提出してください。\n",
    "  - それに対応するpythonのコードをsubmission_code.pyとして提出してください (%%writefileコマンドなどを利用してください)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価方法\n",
    "\n",
    "- 予測ラベルのt_testに対する精度 (Accuracy) で評価します。\n",
    "- 毎日夜24時にテストデータの一部に対する精度でLeader Boardを更新します。\n",
    "- 締切日の夜24時にテストデータ全体に対する精度でLeader Boardを更新します。これを最終的な評価とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み\n",
    "\n",
    "- この部分は修正しないでください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==1.8\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/fa/01883fee1cdb4682bbd188edc26da5982c459e681543bb7f99299fca8800/tensorflow_gpu-1.8.0-cp35-cp35m-manylinux1_x86_64.whl (216.3MB)\n",
      "\u001b[K    100% |################################| 216.3MB 6.9kB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): protobuf>=3.4.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu==1.8)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu==1.8)\n",
      "Requirement already satisfied (use --upgrade to upgrade): gast>=0.2.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu==1.8)\n",
      "Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow-gpu==1.8)\n",
      "  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |################################| 3.1MB 519kB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): absl-py>=0.1.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu==1.8)\n",
      "Requirement already satisfied (use --upgrade to upgrade): grpcio>=1.8.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu==1.8)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.13.3 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu==1.8)\n",
      "Requirement already satisfied (use --upgrade to upgrade): termcolor>=1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu==1.8)\n",
      "Requirement already satisfied (use --upgrade to upgrade): wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow-gpu==1.8)\n",
      "Requirement already satisfied (use --upgrade to upgrade): astor>=0.6.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu==1.8)\n",
      "Requirement already satisfied (use --upgrade to upgrade): setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.4.0->tensorflow-gpu==1.8)\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied (use --upgrade to upgrade): markdown>=2.6.8 in /usr/local/lib/python3.5/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8)\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |################################| 890kB 1.8MB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): werkzeug>=0.11.10 in /usr/local/lib/python3.5/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8)\n",
      "Building wheels for collected packages: html5lib\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
      "Successfully built html5lib\n",
      "Installing collected packages: html5lib, bleach, tensorboard, tensorflow-gpu\n",
      "  Found existing installation: bleach 3.1.0\n",
      "    Uninstalling bleach-3.1.0:\n",
      "      Successfully uninstalled bleach-3.1.0\n",
      "  Found existing installation: tensorboard 1.12.2\n",
      "    Uninstalling tensorboard-1.12.2:\n",
      "      Successfully uninstalled tensorboard-1.12.2\n",
      "  Found existing installation: tensorflow-gpu 1.12.0\n",
      "    Uninstalling tensorflow-gpu-1.12.0:\n",
      "      Successfully uninstalled tensorflow-gpu-1.12.0\n",
      "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-gpu-1.8.0\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-gpu==1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    del [\n",
    "        tf.app,\n",
    "        tf.compat,\n",
    "        tf.contrib,\n",
    "        tf.estimator,\n",
    "        tf.gfile,\n",
    "        tf.graph_util,\n",
    "        tf.image,\n",
    "        tf.initializers,\n",
    "        tf.keras,\n",
    "        tf.layers,\n",
    "        tf.logging,\n",
    "        tf.losses,\n",
    "        tf.metrics,\n",
    "        tf.python_io,\n",
    "        tf.resource_loader,\n",
    "        tf.saved_model,\n",
    "        tf.sets,\n",
    "        tf.summary,\n",
    "        tf.sysconfig,\n",
    "        tf.test\n",
    "    ]\n",
    "    \n",
    "except AttributeError:\n",
    "    print('Unrequired modules are already deleted (Skipped).')\n",
    "\n",
    "def load_mnist():\n",
    "\n",
    "    # 学習データ\n",
    "    x_train = np.load('/root/userspace/public/chap06/data/x_train.npy')\n",
    "    t_train = np.load('/root/userspace/public/chap06/data/t_train.npy')\n",
    "    \n",
    "    # テストデータ\n",
    "    x_test = np.load('/root/userspace/public/chap06/data/x_test.npy')\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "    t_train = np.eye(10)[t_train.astype('int32').flatten()]\n",
    "\n",
    "    return (x_train, x_test, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳み込みニューラルネットワーク(CNN)の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Validation_Cost: 2.224, Accuracy_Score: 0.257\n",
      "EPOCH: 2, Validation_Cost: 1.482, Accuracy_Score: 0.531\n",
      "EPOCH: 3, Validation_Cost: 0.931, Accuracy_Score: 0.646\n",
      "EPOCH: 4, Validation_Cost: 0.804, Accuracy_Score: 0.703\n",
      "EPOCH: 5, Validation_Cost: 0.691, Accuracy_Score: 0.738\n",
      "EPOCH: 6, Validation_Cost: 0.627, Accuracy_Score: 0.761\n",
      "EPOCH: 7, Validation_Cost: 0.582, Accuracy_Score: 0.780\n",
      "EPOCH: 8, Validation_Cost: 0.549, Accuracy_Score: 0.795\n",
      "EPOCH: 9, Validation_Cost: 0.523, Accuracy_Score: 0.807\n",
      "EPOCH: 10, Validation_Cost: 0.502, Accuracy_Score: 0.817\n"
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap06/materials/submission_code.py\n",
    "\n",
    "# import tensorboard as tb\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "\n",
    "### レイヤー定義 ###\n",
    "\n",
    "class Conv:\n",
    "    def __init__(self, filter_shape, function = lambda x: x, strides = [1,1,1,1], padding = 'VALID'):\n",
    "        # He initializationを使う\n",
    "        # filter_shape = Height * Width * Num of input_channels * Num of output_channels\n",
    "        fun_in = np.prod(filter_shape[:3])\n",
    "        fun_out = np.prod(filter_shape[:2]) * filter_shape[3]\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = -np.sqrt(6/ fun_in),\n",
    "                high = np.sqrt(6/ fun_out),\n",
    "                size = filter_shape\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((filter_shape[3]), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    # WRITE ME\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        u = tf.nn.conv2d(x, self.W, strides = self.strides, padding = self.padding) + self.b\n",
    "        return self.function(u)\n",
    "    \n",
    "    \n",
    "class Pooling:\n",
    "    def __init__(self, ksize = [1, 2, 2, 1] , strides = [1, 2, 2, 1], padding = 'VALID'):\n",
    "        self.ksize = ksize\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.max_pool(x, ksize = self.ksize, strides = self.strides, padding = self.padding)\n",
    "    # WRITE ME\n",
    "    \n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        return tf.reshape(x, (-1, np.prod(x.get_shape().as_list()[1:])))\n",
    "    # WRITE ME\n",
    "    \n",
    "    \n",
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function = lambda x: x):\n",
    "        # ここでも, He Initialization\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = - np.sqrt(6/ in_dim),\n",
    "                high = np.sqrt(6/ in_dim),\n",
    "                size = [in_dim, out_dim]\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((out_dim), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        u = tf.matmul(x, self.W) + self.b\n",
    "        return self.function(u)\n",
    "    \n",
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))\n",
    "    # WRITE ME\n",
    "    \n",
    "\n",
    "\n",
    "x_train, x_test, t_train = load_mnist()\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.1, random_state=random_state)\n",
    "\n",
    "### ネットワーク ###\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "                                                                          \n",
    "h = Conv((5, 5, 1, 20), tf.nn.relu)(x)\n",
    "h = Pooling((1, 2, 2, 1))(h)\n",
    "h = Conv((5, 5, 20, 50), tf.nn.relu)(h)\n",
    "h = Pooling((1, 2, 2, 1))(h)\n",
    "h = Flatten()(h)\n",
    "y = Dense(800, 10, tf.nn.softmax)(h)\n",
    "\n",
    "cost = - tf.reduce_mean(tf.reduce_sum(t * tf_log(y), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "### 学習 ###\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = x_train.shape[0]//batch_size #Floor division(打ち切り除算)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        x_train_fmnist, t_train_fmnist = shuffle(x_train, t_train, random_state = random_state)\n",
    "        for batch in range(n_batches):\n",
    "            start = batch * batch_size\n",
    "            finish = start + batch_size\n",
    "            sess.run(train, feed_dict = {x: x_train_fmnist[start:finish], t:t_train_fmnist[start:finish]})\n",
    "        y_pred_, valid_cost_ = sess.run([y, cost], \n",
    "                            feed_dict = {x: x_valid, t:t_valid}\n",
    "                            )\n",
    "        print(\"EPOCH: {}, Validation_Cost: {:.3f}, Accuracy_Score: {:.3f}\".format(\n",
    "            epoch+1,\n",
    "            valid_cost_,\n",
    "            accuracy_score(t_valid.argmax(axis = 1), y_pred_.argmax(axis = 1))\n",
    "        ))\n",
    "    \n",
    "    y_pred = sess.run(y, feed_dict = {x: x_test})\n",
    "    submission = pd.Series(y_pred.argmax(axis = 1), name='label')\n",
    "    submission.to_csv('/root/userspace/chap06/submission/submission_pred_trial.csv', header=True, index_label='id')\n",
    "\n",
    "# tb.show_graph(tf.get_default_graph().as_graph_def())\n",
    "\n",
    "# WRITE ME\n",
    "# submission = pd.Series(y_pred, name='label')\n",
    "# submission.to_csv('/root/userspace/chap06/materials/submission_pred.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet(1998)の実装\n",
    "具体的には以下のようになる。\n",
    "1. Convolution\n",
    "2. Max pooling\n",
    "3. Convolution\n",
    "4. Max pooling  \n",
    "*以下、MLP*\n",
    "5. Full connection layer(120 neurons)\n",
    "6. Full connection layer(84 neurons)\n",
    "7. Output(10 neurons)\n",
    "\n",
    "```python\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力次元数\n",
    "INPUT: $28 \\times 28 \\times 1$  \n",
    "C1: $24 \\times 24 \\times 6$  \n",
    "S2: $12 \\times 12 \\times 6$  \n",
    "C3: $8 \\times 8 \\times 16$  \n",
    "S4: $4 \\times 4 \\times 16 \\rightarrow 256$(after \"Flatten\" is applied)  \n",
    "C5: $120$  \n",
    "C6: $84$  \n",
    "OUTPUT: $10$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Flatten, the shape of h is:(?, 4, 4, 16)\n",
      "After Flatten, the shape of h is:(?, 256)\n",
      "EPOCH: 1, Validation_Cost: 0.460, Accuracy_Score: 0.840\n",
      "EPOCH: 2, Validation_Cost: 0.437, Accuracy_Score: 0.851\n",
      "EPOCH: 3, Validation_Cost: 0.421, Accuracy_Score: 0.858\n",
      "EPOCH: 4, Validation_Cost: 0.400, Accuracy_Score: 0.866\n",
      "EPOCH: 5, Validation_Cost: 0.396, Accuracy_Score: 0.868\n",
      "EPOCH: 6, Validation_Cost: 0.398, Accuracy_Score: 0.871\n",
      "EPOCH: 7, Validation_Cost: 0.386, Accuracy_Score: 0.876\n",
      "EPOCH: 8, Validation_Cost: 0.401, Accuracy_Score: 0.873\n",
      "EPOCH: 9, Validation_Cost: 0.399, Accuracy_Score: 0.873\n",
      "EPOCH: 10, Validation_Cost: 0.398, Accuracy_Score: 0.873\n",
      "EPOCH: 11, Validation_Cost: 0.390, Accuracy_Score: 0.872\n",
      "EPOCH: 12, Validation_Cost: 0.400, Accuracy_Score: 0.872\n",
      "EPOCH: 13, Validation_Cost: 0.399, Accuracy_Score: 0.874\n",
      "EPOCH: 14, Validation_Cost: 0.408, Accuracy_Score: 0.868\n",
      "EPOCH: 15, Validation_Cost: 0.416, Accuracy_Score: 0.869\n",
      "EPOCH: 16, Validation_Cost: 0.407, Accuracy_Score: 0.872\n",
      "EPOCH: 17, Validation_Cost: 0.427, Accuracy_Score: 0.868\n",
      "EPOCH: 18, Validation_Cost: 0.394, Accuracy_Score: 0.873\n",
      "EPOCH: 19, Validation_Cost: 0.400, Accuracy_Score: 0.873\n",
      "EPOCH: 20, Validation_Cost: 0.408, Accuracy_Score: 0.870\n",
      "EPOCH: 21, Validation_Cost: 0.403, Accuracy_Score: 0.870\n",
      "EPOCH: 22, Validation_Cost: 0.425, Accuracy_Score: 0.864\n",
      "EPOCH: 23, Validation_Cost: 0.395, Accuracy_Score: 0.876\n",
      "EPOCH: 24, Validation_Cost: 0.406, Accuracy_Score: 0.871\n"
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap06/materials/submission_code.py\n",
    "\n",
    "# import tensorboard as tb\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "\n",
    "### レイヤー定義 ###\n",
    "\n",
    "class Conv:\n",
    "    def __init__(self, filter_shape, function = lambda x: x, strides = [1,1,1,1], padding = 'VALID'):\n",
    "        # He initializationを使う\n",
    "        # filter_shape = Height * Width * Num of input_channels * Num of output_channels\n",
    "        fun_in = np.prod(filter_shape[:3])\n",
    "        fun_out = np.prod(filter_shape[:2]) * filter_shape[3]\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = -np.sqrt(6/ fun_in),\n",
    "                high = np.sqrt(6/ fun_out),\n",
    "                size = filter_shape\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((filter_shape[3]), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    # WRITE ME\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        u = tf.nn.conv2d(x, self.W, strides = self.strides, padding = self.padding) + self.b\n",
    "        return self.function(u)\n",
    "    \n",
    "    \n",
    "class Pooling:\n",
    "    def __init__(self, ksize = [1, 2, 2, 1] , strides = [1, 2, 2, 1], padding = 'VALID'):\n",
    "        self.ksize = ksize\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.max_pool(x, ksize = self.ksize, strides = self.strides, padding = self.padding)\n",
    "    # WRITE ME\n",
    "    \n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        return tf.reshape(x, (-1, np.prod(x.get_shape().as_list()[1:])))\n",
    "    # WRITE ME\n",
    "    \n",
    "    \n",
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function = lambda x: x):\n",
    "        # ここでも, He Initialization\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = - np.sqrt(6/ in_dim),\n",
    "                high = np.sqrt(6/ in_dim),\n",
    "                size = [in_dim, out_dim]\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((out_dim), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        u = tf.matmul(x, self.W) + self.b\n",
    "        return self.function(u)\n",
    "    \n",
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))\n",
    "    # WRITE ME\n",
    "\n",
    "def get_params(layers):\n",
    "    params_all = []\n",
    "    for layer in layers:\n",
    "        params = layer.params\n",
    "        params_all.extend(params)\n",
    "    return params_all\n",
    "    \n",
    "def compute_l1_reg(params):\n",
    "    l1_reg = 0\n",
    "    for param in params:\n",
    "        l1_reg += tf.reduce_sum(tf.abs(param))\n",
    "    return l1_reg\n",
    "\n",
    "def compute_l2_reg(params):\n",
    "    l2_reg = 0\n",
    "    for param in params:\n",
    "        l2_reg += tf.reduce_sum(tf.square(param)) # 2 * tf.nn.l2_lossを使っても良い\n",
    "    return l2_reg\n",
    "\n",
    "x_train, x_test, t_train = load_mnist()\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.1, random_state=random_state)\n",
    "\n",
    "### ネットワーク ###\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "lmd = 0.0001\n",
    "\n",
    "# LeNetっぽくやってみる\n",
    "\n",
    "# L2正規化のために、全結合層だけ名前をつけておく。\n",
    "C5 = Dense(256, 120, tf.nn.relu)\n",
    "C6 = Dense(120, 84, tf.nn.relu)\n",
    "\n",
    "h = Conv((5, 5, 1, 6), tf.nn.relu)(x) \n",
    "h = Pooling((1, 2, 2, 1))(h)\n",
    "h = Conv((5, 5, 6, 16), tf.nn.relu)(h)\n",
    "h = Pooling((1, 2, 2, 1))(h)\n",
    "print(\"Before Flatten, the shape of h is:{}\".format(h.shape))\n",
    "h = Flatten()(h)\n",
    "print(\"After Flatten, the shape of h is:{}\".format(h.shape))\n",
    "h = C5(h)\n",
    "h = C6(h)\n",
    "y = Dense(84, 10, tf.nn.softmax)(h)\n",
    "\n",
    "\n",
    "layers = [C5, C6]\n",
    "params_all = get_params(layers)\n",
    "l2reg = compute_l2_reg(params_all)\n",
    "\n",
    "\n",
    "cost = - tf.reduce_mean(tf.reduce_sum(t * tf_log(y), axis=1)) + lmd * l2reg\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "### 学習 ###\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = x_train.shape[0]//batch_size #Floor division(打ち切り除算)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        x_train_fmnist, t_train_fmnist = shuffle(x_train, t_train, random_state = random_state)\n",
    "        for batch in range(n_batches):\n",
    "            start = batch * batch_size\n",
    "            finish = start + batch_size\n",
    "            sess.run(train, feed_dict = {x: x_train_fmnist[start:finish], t:t_train_fmnist[start:finish]})\n",
    "        y_pred_, valid_cost_ = sess.run([y, cost], \n",
    "                            feed_dict = {x: x_valid, t:t_valid}\n",
    "                            )\n",
    "        print(\"EPOCH: {}, Validation_Cost: {:.3f}, Accuracy_Score: {:.3f}\".format(\n",
    "            epoch+1,\n",
    "            valid_cost_,\n",
    "            accuracy_score(t_valid.argmax(axis = 1), y_pred_.argmax(axis = 1))\n",
    "        ))\n",
    "    \n",
    "    y_pred = sess.run(y, feed_dict = {x: x_test})\n",
    "    submission = pd.Series(y_pred.argmax(axis = 1), name='label')\n",
    "    submission.to_csv('/root/userspace/chap06/submission/submission_pred_LeNet_L2reg.csv', header=True, index_label='id')\n",
    "\n",
    "# tb.show_graph(tf.get_default_graph().as_graph_def())\n",
    "\n",
    "# WRITE ME\n",
    "# submission = pd.Series(y_pred, name='label')\n",
    "# submission.to_csv('/root/userspace/chap06/materials/submission_pred.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniVGG netを実装する\n",
    "[このサイト](http://cedro3.com/ai/mini-vgg-net/)などを参考にMiniVGGを実装してみることにする。  \n",
    "MiniVGGのArchitectureは以下の表のようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MiniVGG](http://cedro3.com/wp-content/uploads/2017/12/berore2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### バッチ正規化(Batch Normalization)\n",
    "しかしながら、実用上このような深いニューラルネットワークに対してはバッチ正規化を行うことが推奨されている。  \n",
    "そこで、次のようにBatch Normalizationの処理を挿入していく。  \n",
    "Batch Normalizationの処理の意味はいまいちよく理解出来ていないのだが、DL本や[このページ](https://qiita.com/cfiken/items/b477c7878828ebdb0387)で頑張って解説されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Batch Normalization](http://cedro3.com/wp-content/uploads/2017/12/mark2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Flatten, the shape of h is:(?, 4, 4, 64)\n",
      "After Flatten, the shape of h is:(?, 1024)\n",
      "EPOCH: 5, Validation_Cost: 0.440, Accuracy_Score: 0.833\n",
      "EPOCH: 10, Validation_Cost: 0.358, Accuracy_Score: 0.869\n",
      "EPOCH: 15, Validation_Cost: 0.334, Accuracy_Score: 0.871\n",
      "EPOCH: 20, Validation_Cost: 0.304, Accuracy_Score: 0.888\n",
      "EPOCH: 25, Validation_Cost: 0.321, Accuracy_Score: 0.884\n",
      "EPOCH: 30, Validation_Cost: 0.300, Accuracy_Score: 0.890\n",
      "EPOCH: 35, Validation_Cost: 0.292, Accuracy_Score: 0.893\n",
      "EPOCH: 40, Validation_Cost: 0.285, Accuracy_Score: 0.894\n",
      "EPOCH: 45, Validation_Cost: 0.278, Accuracy_Score: 0.898\n",
      "EPOCH: 50, Validation_Cost: 0.289, Accuracy_Score: 0.896\n",
      "EPOCH: 55, Validation_Cost: 0.282, Accuracy_Score: 0.902\n",
      "EPOCH: 60, Validation_Cost: 0.268, Accuracy_Score: 0.901\n",
      "EPOCH: 65, Validation_Cost: 0.272, Accuracy_Score: 0.905\n",
      "EPOCH: 70, Validation_Cost: 0.274, Accuracy_Score: 0.904\n",
      "EPOCH: 75, Validation_Cost: 0.285, Accuracy_Score: 0.902\n",
      "EPOCH: 80, Validation_Cost: 0.278, Accuracy_Score: 0.902\n",
      "EPOCH: 85, Validation_Cost: 0.277, Accuracy_Score: 0.905\n",
      "EPOCH: 90, Validation_Cost: 0.265, Accuracy_Score: 0.908\n",
      "EPOCH: 95, Validation_Cost: 0.292, Accuracy_Score: 0.901\n",
      "EPOCH: 100, Validation_Cost: 0.271, Accuracy_Score: 0.908\n"
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap06/materials/submission_code.py\n",
    "\n",
    "# import tensorboard as tb\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "\n",
    "### レイヤー定義 ###\n",
    "\n",
    "class Conv:\n",
    "    def __init__(self, filter_shape, function = lambda x: x, strides = [1,1,1,1], padding = 'VALID'):\n",
    "        # He initializationを使う\n",
    "        # filter_shape = Height * Width * Num of input_channels * Num of output_channels\n",
    "        fun_in = np.prod(filter_shape[:3])\n",
    "        fun_out = np.prod(filter_shape[:2]) * filter_shape[3]\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = -np.sqrt(6/ fun_in),\n",
    "                high = np.sqrt(6/ fun_out),\n",
    "                size = filter_shape\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((filter_shape[3]), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    # WRITE ME\n",
    "    \n",
    "    def __call__(self, x, config):\n",
    "        u = tf.nn.conv2d(x, self.W, strides = self.strides, padding = self.padding) + self.b\n",
    "        self.moments = tf.nn.moments(u, axes = [0], name = 'mean', keep_dims = True)\n",
    "        u = tf.cond(\n",
    "        pred = config,\n",
    "        true_fn = lambda: tf.nn.batch_normalization(u, \n",
    "                                         mean = self.moments[0], \n",
    "                                         variance = self.moments[1],\n",
    "                                        offset = None,\n",
    "                                        scale = None,\n",
    "                                        variance_epsilon = 1e-8),\n",
    "        false_fn = lambda: u)\n",
    "        return self.function(u)\n",
    "    \n",
    "class Pooling:\n",
    "    def __init__(self, ksize = [1, 2, 2, 1] , strides = [1, 2, 2, 1], padding = 'VALID'):\n",
    "        self.ksize = ksize\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.max_pool(x, ksize = self.ksize, strides = self.strides, padding = self.padding)\n",
    "    # WRITE ME\n",
    "    \n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        return tf.reshape(x, (-1, np.prod(x.get_shape().as_list()[1:])))\n",
    "    # WRITE ME\n",
    "    \n",
    "    \n",
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function = lambda x: x):\n",
    "        # ここでも, He Initialization\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = - np.sqrt(6/ in_dim),\n",
    "                high = np.sqrt(6/ in_dim),\n",
    "                size = [in_dim, out_dim]\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((out_dim), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        u = tf.matmul(x, self.W) + self.b\n",
    "        return self.function(u)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_keep_prob=1.0):\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.params = []\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # 訓練時のみdropoutを適用\n",
    "        return tf.cond(\n",
    "            pred=is_training,\n",
    "            true_fn=lambda: tf.nn.dropout(x, keep_prob=self.dropout_keep_prob),\n",
    "            false_fn=lambda: x\n",
    "        )\n",
    "    \n",
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))\n",
    "    # WRITE ME\n",
    "\n",
    "def get_params(layers):\n",
    "    params_all = []\n",
    "    for layer in layers:\n",
    "        params = layer.params\n",
    "        params_all.extend(params)\n",
    "    return params_all\n",
    "    \n",
    "def compute_l1_reg(params):\n",
    "    l1_reg = 0\n",
    "    for param in params:\n",
    "        l1_reg += tf.reduce_sum(tf.abs(param))\n",
    "    return l1_reg\n",
    "\n",
    "def compute_l2_reg(params):\n",
    "    l2_reg = 0\n",
    "    for param in params:\n",
    "        l2_reg += tf.reduce_sum(tf.square(param)) # 2 * tf.nn.l2_lossを使っても良い\n",
    "    return l2_reg\n",
    "\n",
    "x_train, x_test, t_train = load_mnist()\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.1, random_state=random_state)\n",
    "\n",
    "### ネットワーク ###\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "lmd = 0.0001\n",
    "dropout_keep_prob = 0.75\n",
    "\n",
    "\n",
    "\n",
    "# # miniVGG network\n",
    "# config = tf.Variable(True)\n",
    "\n",
    "# h = Conv(filter_shape = (5, 5, 1, 6), function = tf.nn.relu)(x, config) # [None, 28, 28, 1] -> [None, 24, 24, 6]\n",
    "# h = Conv(filter_shape = (3, 3, 6, 6), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 24, 24, 6] -> [None, 24, 24, 6]\n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 24, 24, 6] -> [None, 12, 12, 6]\n",
    "\n",
    "# h = Conv(filter_shape = (5, 5, 6, 16), function = tf.nn.relu)(h, config) # [None, 12, 12, 6] -> [None, 8, 8, 16]\n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 8, 8, 16] -> [None, 4, 4, 16]\n",
    "\n",
    "# print(\"Before Flatten, the shape of h is:{}\".format(h.shape))\n",
    "# h = Flatten()(h)\n",
    "# print(\"After Flatten, the shape of h is:{}\".format(h.shape))\n",
    "\n",
    "# h = Dense(256, 120, tf.nn.relu)(h)\n",
    "# h = Dense(120, 84, tf.nn.relu)(h)\n",
    "# y = Dense(84, 10, tf.nn.softmax)(h)\n",
    "\n",
    "\n",
    "# miniVGG network\n",
    "config = tf.Variable(True)\n",
    "\n",
    "h = Conv(filter_shape = (3, 3, 1, 32), function = tf.nn.relu, padding = 'SAME')(x, config) # [None, 28, 28, 1] -> [None, 28, 28, 32]\n",
    "h = Conv(filter_shape = (3, 3, 32, 32), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 28, 28, 32] -> [None, 28, 28, 32]\n",
    "h = Conv(filter_shape = (3, 3, 32, 32), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 28, 28, 32] -> [None, 28, 28, 32]\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 28, 28, 32] -> [None, 14, 14, 32]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "h = Conv(filter_shape = (3, 3, 32, 64), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 14, 14, 32] -> [None, 14, 14, 64]\n",
    "h = Conv(filter_shape = (3, 3, 64, 64), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 14, 14, 64] -> [None, 14, 14, 64]\n",
    "h = Conv(filter_shape = (3, 3, 64, 64), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 14, 14, 64] -> [None, 14, 14, 64]\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 14, 14, 64] -> [None, 7, 7, 64]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "h = Conv(filter_shape = (3, 3, 64, 64), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 7, 7, 64] -> [None, 7, 7, 64]\n",
    "h = Conv(filter_shape = (3, 3, 64, 64), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 7, 7, 64] -> [None, 7, 7, 64]\n",
    "h = Conv(filter_shape = (3, 3, 64, 64), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 7, 7, 64] -> [None, 7, 7, 64]\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1], padding = 'SAME')(h) # [None, 7, 7, 64] -> [None, 4, 4, 64]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "print(\"Before Flatten, the shape of h is:{}\".format(h.shape))\n",
    "h = Flatten()(h)\n",
    "print(\"After Flatten, the shape of h is:{}\".format(h.shape))\n",
    "\n",
    "h = Dense(1024, 512, tf.nn.relu)(h)\n",
    "h = Dropout(0.5)(h)\n",
    "# h = Dense(512, 84, tf.nn.relu)(h)\n",
    "# h = Dropout(0.5)(h)\n",
    "y = Dense(512, 10, tf.nn.softmax)(h)\n",
    "\n",
    "\n",
    "cost = - tf.reduce_mean(tf.reduce_sum(t * tf_log(y), axis=1))\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "# optimizer = tf.train.AdadeltaOptimizer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "    \n",
    "# 単純に移動平均・移動分散を計算するだけではパラメータが更新されないので、以下のように書き換える必要あり。\n",
    "# crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.cnn.y, logits=self.cnn.logits)\n",
    "# loss_op = tf.reduce_mean(crossent)\n",
    "# optimizer = tf.train.AdamOptimizer(config['learning_rate'])\n",
    "# extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  # <- ここ\n",
    "# with tf.control_dependencies(extra_update_ops):  # <- ここ\n",
    "#     train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "### 学習 ###\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "n_batches = x_train.shape[0]//batch_size #Floor division(打ち切り除算)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        x_train_fmnist, t_train_fmnist = shuffle(x_train, t_train, random_state = random_state)\n",
    "        for batch in range(n_batches):\n",
    "            start = batch * batch_size\n",
    "            finish = start + batch_size\n",
    "            sess.run(train, feed_dict = {x: x_train_fmnist[start:finish], t:t_train_fmnist[start:finish], is_training:True})\n",
    "        y_pred_, valid_cost_ = sess.run([y, cost], \n",
    "                            feed_dict = {x: x_valid, t:t_valid, is_training:False}\n",
    "                            )\n",
    "        if ((epoch+1) % 5 == 0):\n",
    "            print(\"EPOCH: {}, Validation_Cost: {:.3f}, Accuracy_Score: {:.3f}\".format(\n",
    "                epoch+1,\n",
    "                valid_cost_,\n",
    "                accuracy_score(t_valid.argmax(axis = 1), y_pred_.argmax(axis = 1))\n",
    "            ))\n",
    "    y_pred = sess.run(y, feed_dict = {x: x_test, is_training: False})\n",
    "    submission = pd.Series(y_pred.argmax(axis = 1), name='label')\n",
    "    submission.to_csv('/root/userspace/chap06/submission/submission_pred_miniVGG_bn_adadelta.csv', header=True, index_label='id')\n",
    "\n",
    "# tb.show_graph(tf.get_default_graph().as_graph_def())\n",
    "\n",
    "# WRITE ME\n",
    "# submission = pd.Series(y_pred, name='label')\n",
    "# submission.to_csv('/root/userspace/chap06/materials/submission_pred.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果\n",
    "`submission_pred_miniVGG_bn`は全てのbatch normalizationとDropoutを実行したminiVGGNetでの結果。\n",
    "100Epochほど回し、最終的なAccuracy_Scoreは0.918であった。  \n",
    "`submission_pred_miniVGG_bn_keep`はConvolution層のDropoutを無くした場合。\n",
    "100Epoch回したが、最終的なAccuracy_Scoreは0.877で悪化した。  \n",
    "`submission_pred_miniVGG_bn_alldropout.csv`は全てにDropoutを実行したもの\n",
    "最終的なAccuracy_scoreは0.910であった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizerの変更\n",
    "上記は全てAdamで実行したが、他のOptimizerも試してみる。\n",
    "- Adadeltaの結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16の実装\n",
    "[このサイト](https://zhuanlan.zhihu.com/p/28968219)を参考にして、VGG16を実装した。  \n",
    "ファイル名は`submission_pred_VGG16_bn_adam.csv`で保存した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Flatten, the shape of h is:(?, 2, 2, 512)\n",
      "After Flatten, the shape of h is:(?, 2048)\n",
      "EPOCH: 1, Validation_Cost: 1.078, Accuracy_Score: 0.627\n",
      "EPOCH: 2, Validation_Cost: 1.112, Accuracy_Score: 0.623\n",
      "EPOCH: 3, Validation_Cost: 0.975, Accuracy_Score: 0.679\n",
      "EPOCH: 4, Validation_Cost: 1.017, Accuracy_Score: 0.653\n",
      "EPOCH: 5, Validation_Cost: 1.047, Accuracy_Score: 0.691\n",
      "EPOCH: 6, Validation_Cost: 0.781, Accuracy_Score: 0.723\n",
      "EPOCH: 7, Validation_Cost: 0.630, Accuracy_Score: 0.780\n",
      "EPOCH: 8, Validation_Cost: 0.587, Accuracy_Score: 0.755\n",
      "EPOCH: 9, Validation_Cost: 0.539, Accuracy_Score: 0.766\n",
      "EPOCH: 10, Validation_Cost: 0.553, Accuracy_Score: 0.771\n",
      "EPOCH: 11, Validation_Cost: 0.553, Accuracy_Score: 0.769\n",
      "EPOCH: 12, Validation_Cost: 0.492, Accuracy_Score: 0.791\n",
      "EPOCH: 13, Validation_Cost: 0.480, Accuracy_Score: 0.797\n",
      "EPOCH: 14, Validation_Cost: 0.485, Accuracy_Score: 0.792\n",
      "EPOCH: 15, Validation_Cost: 0.554, Accuracy_Score: 0.779\n",
      "EPOCH: 16, Validation_Cost: 0.462, Accuracy_Score: 0.803\n",
      "EPOCH: 17, Validation_Cost: 0.461, Accuracy_Score: 0.804\n",
      "EPOCH: 18, Validation_Cost: 0.460, Accuracy_Score: 0.804\n",
      "EPOCH: 19, Validation_Cost: 0.473, Accuracy_Score: 0.804\n",
      "EPOCH: 20, Validation_Cost: 0.469, Accuracy_Score: 0.796\n",
      "EPOCH: 21, Validation_Cost: 0.542, Accuracy_Score: 0.768\n",
      "EPOCH: 22, Validation_Cost: 0.409, Accuracy_Score: 0.810\n",
      "EPOCH: 23, Validation_Cost: 0.412, Accuracy_Score: 0.811\n",
      "EPOCH: 24, Validation_Cost: 0.399, Accuracy_Score: 0.810\n",
      "EPOCH: 25, Validation_Cost: 0.394, Accuracy_Score: 0.815\n",
      "EPOCH: 26, Validation_Cost: 0.417, Accuracy_Score: 0.825\n",
      "EPOCH: 27, Validation_Cost: 0.334, Accuracy_Score: 0.901\n",
      "EPOCH: 28, Validation_Cost: 0.368, Accuracy_Score: 0.894\n",
      "EPOCH: 29, Validation_Cost: 0.310, Accuracy_Score: 0.897\n",
      "EPOCH: 30, Validation_Cost: 0.285, Accuracy_Score: 0.905\n",
      "EPOCH: 31, Validation_Cost: 0.289, Accuracy_Score: 0.902\n",
      "EPOCH: 32, Validation_Cost: 0.312, Accuracy_Score: 0.907\n",
      "EPOCH: 33, Validation_Cost: 0.301, Accuracy_Score: 0.910\n",
      "EPOCH: 34, Validation_Cost: 0.254, Accuracy_Score: 0.914\n",
      "EPOCH: 35, Validation_Cost: 0.358, Accuracy_Score: 0.893\n",
      "EPOCH: 36, Validation_Cost: 0.282, Accuracy_Score: 0.910\n",
      "EPOCH: 37, Validation_Cost: 0.268, Accuracy_Score: 0.912\n",
      "EPOCH: 38, Validation_Cost: 0.277, Accuracy_Score: 0.914\n",
      "EPOCH: 39, Validation_Cost: 0.278, Accuracy_Score: 0.911\n",
      "EPOCH: 40, Validation_Cost: 0.261, Accuracy_Score: 0.916\n",
      "EPOCH: 41, Validation_Cost: 0.267, Accuracy_Score: 0.913\n",
      "EPOCH: 42, Validation_Cost: 0.258, Accuracy_Score: 0.919\n",
      "EPOCH: 43, Validation_Cost: 0.270, Accuracy_Score: 0.911\n",
      "EPOCH: 44, Validation_Cost: 0.259, Accuracy_Score: 0.918\n",
      "EPOCH: 45, Validation_Cost: 0.262, Accuracy_Score: 0.913\n",
      "EPOCH: 46, Validation_Cost: 0.259, Accuracy_Score: 0.916\n",
      "EPOCH: 47, Validation_Cost: 0.253, Accuracy_Score: 0.920\n",
      "EPOCH: 48, Validation_Cost: 0.249, Accuracy_Score: 0.919\n",
      "EPOCH: 49, Validation_Cost: 0.245, Accuracy_Score: 0.919\n",
      "EPOCH: 50, Validation_Cost: 0.254, Accuracy_Score: 0.920\n",
      "EPOCH: 51, Validation_Cost: 0.253, Accuracy_Score: 0.924\n",
      "EPOCH: 52, Validation_Cost: 0.264, Accuracy_Score: 0.918\n",
      "EPOCH: 53, Validation_Cost: 0.255, Accuracy_Score: 0.920\n",
      "EPOCH: 54, Validation_Cost: 0.257, Accuracy_Score: 0.922\n",
      "EPOCH: 55, Validation_Cost: 0.338, Accuracy_Score: 0.900\n",
      "EPOCH: 56, Validation_Cost: 0.254, Accuracy_Score: 0.924\n",
      "EPOCH: 57, Validation_Cost: 0.263, Accuracy_Score: 0.923\n",
      "EPOCH: 58, Validation_Cost: 0.246, Accuracy_Score: 0.926\n",
      "EPOCH: 59, Validation_Cost: 0.257, Accuracy_Score: 0.920\n",
      "EPOCH: 60, Validation_Cost: 0.269, Accuracy_Score: 0.924\n",
      "EPOCH: 61, Validation_Cost: 0.271, Accuracy_Score: 0.918\n",
      "EPOCH: 62, Validation_Cost: 0.275, Accuracy_Score: 0.925\n",
      "EPOCH: 63, Validation_Cost: 0.252, Accuracy_Score: 0.926\n",
      "EPOCH: 64, Validation_Cost: 0.239, Accuracy_Score: 0.928\n",
      "EPOCH: 65, Validation_Cost: 0.282, Accuracy_Score: 0.923\n",
      "EPOCH: 66, Validation_Cost: 0.250, Accuracy_Score: 0.927\n",
      "EPOCH: 67, Validation_Cost: 0.303, Accuracy_Score: 0.909\n",
      "EPOCH: 68, Validation_Cost: 0.258, Accuracy_Score: 0.926\n",
      "EPOCH: 69, Validation_Cost: 0.272, Accuracy_Score: 0.926\n",
      "EPOCH: 70, Validation_Cost: 0.265, Accuracy_Score: 0.927\n",
      "EPOCH: 71, Validation_Cost: 0.264, Accuracy_Score: 0.927\n",
      "EPOCH: 72, Validation_Cost: 0.259, Accuracy_Score: 0.926\n",
      "EPOCH: 73, Validation_Cost: 0.405, Accuracy_Score: 0.911\n",
      "EPOCH: 74, Validation_Cost: 1.399, Accuracy_Score: 0.871\n",
      "EPOCH: 75, Validation_Cost: 0.250, Accuracy_Score: 0.930\n",
      "EPOCH: 76, Validation_Cost: 0.255, Accuracy_Score: 0.929\n",
      "EPOCH: 77, Validation_Cost: 0.255, Accuracy_Score: 0.930\n",
      "EPOCH: 78, Validation_Cost: 0.258, Accuracy_Score: 0.928\n",
      "EPOCH: 79, Validation_Cost: 0.271, Accuracy_Score: 0.928\n",
      "EPOCH: 80, Validation_Cost: 0.280, Accuracy_Score: 0.927\n",
      "EPOCH: 81, Validation_Cost: 0.274, Accuracy_Score: 0.929\n",
      "EPOCH: 82, Validation_Cost: 0.269, Accuracy_Score: 0.924\n",
      "EPOCH: 83, Validation_Cost: 0.251, Accuracy_Score: 0.929\n",
      "EPOCH: 84, Validation_Cost: 0.273, Accuracy_Score: 0.931\n",
      "EPOCH: 85, Validation_Cost: 0.271, Accuracy_Score: 0.931\n",
      "EPOCH: 86, Validation_Cost: 0.317, Accuracy_Score: 0.927\n",
      "EPOCH: 87, Validation_Cost: 0.290, Accuracy_Score: 0.928\n",
      "EPOCH: 88, Validation_Cost: 0.269, Accuracy_Score: 0.930\n",
      "EPOCH: 89, Validation_Cost: 0.292, Accuracy_Score: 0.930\n",
      "EPOCH: 90, Validation_Cost: 0.522, Accuracy_Score: 0.915\n",
      "EPOCH: 91, Validation_Cost: 0.411, Accuracy_Score: 0.895\n",
      "EPOCH: 92, Validation_Cost: 0.705, Accuracy_Score: 0.907\n",
      "EPOCH: 93, Validation_Cost: 0.251, Accuracy_Score: 0.926\n",
      "EPOCH: 94, Validation_Cost: 0.262, Accuracy_Score: 0.930\n",
      "EPOCH: 95, Validation_Cost: 0.260, Accuracy_Score: 0.932\n",
      "EPOCH: 96, Validation_Cost: 0.289, Accuracy_Score: 0.930\n",
      "EPOCH: 97, Validation_Cost: 0.262, Accuracy_Score: 0.929\n",
      "EPOCH: 98, Validation_Cost: 0.466, Accuracy_Score: 0.893\n",
      "EPOCH: 99, Validation_Cost: 0.260, Accuracy_Score: 0.931\n"
     ]
    }
   ],
   "source": [
    "# %%writefile /root/userspace/chap06/materials/submission_code.py\n",
    "\n",
    "# import tensorboard as tb\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "\n",
    "### レイヤー定義 ###\n",
    "\n",
    "class Conv:\n",
    "    def __init__(self, filter_shape, function = lambda x: x, strides = [1,1,1,1], padding = 'VALID'):\n",
    "        # He initializationを使う\n",
    "        # filter_shape = Height * Width * Num of input_channels * Num of output_channels\n",
    "        fun_in = np.prod(filter_shape[:3])\n",
    "        fun_out = np.prod(filter_shape[:2]) * filter_shape[3]\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = -np.sqrt(6/ fun_in),\n",
    "                high = np.sqrt(6/ fun_out),\n",
    "                size = filter_shape\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((filter_shape[3]), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    # WRITE ME\n",
    "    \n",
    "    def __call__(self, x, config):\n",
    "        u = tf.nn.conv2d(x, self.W, strides = self.strides, padding = self.padding) + self.b\n",
    "        self.moments = tf.nn.moments(u, axes = [0], name = 'mean', keep_dims = True)\n",
    "        u = tf.cond(\n",
    "        pred = config,\n",
    "        true_fn = lambda: tf.nn.batch_normalization(u, \n",
    "                                         mean = self.moments[0], \n",
    "                                         variance = self.moments[1],\n",
    "                                        offset = None,\n",
    "                                        scale = None,\n",
    "                                        variance_epsilon = 1e-8),\n",
    "        false_fn = lambda: u)\n",
    "        return self.function(u)\n",
    "    \n",
    "class Pooling:\n",
    "    def __init__(self, ksize = [1, 2, 2, 1] , strides = [1, 2, 2, 1], padding = 'VALID'):\n",
    "        self.ksize = ksize\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.max_pool(x, ksize = self.ksize, strides = self.strides, padding = self.padding)\n",
    "    # WRITE ME\n",
    "    \n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        return tf.reshape(x, (-1, np.prod(x.get_shape().as_list()[1:])))\n",
    "    # WRITE ME\n",
    "    \n",
    "    \n",
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function = lambda x: x):\n",
    "        # ここでも, He Initialization\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                low = - np.sqrt(6/ in_dim),\n",
    "                high = np.sqrt(6/ in_dim),\n",
    "                size = [in_dim, out_dim]\n",
    "                ).astype('float32'), name = 'W')\n",
    "        self.b = tf.Variable(np.zeros((out_dim), dtype = 'float32'), name = 'b')\n",
    "        self.function = function\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        u = tf.matmul(x, self.W) + self.b\n",
    "        return self.function(u)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_keep_prob=1.0):\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.params = []\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # 訓練時のみdropoutを適用\n",
    "        return tf.cond(\n",
    "            pred=is_training,\n",
    "            true_fn=lambda: tf.nn.dropout(x, keep_prob=self.dropout_keep_prob),\n",
    "            false_fn=lambda: x\n",
    "        )\n",
    "    \n",
    "def tf_log(x):\n",
    "    return tf.log(tf.clip_by_value(x, 1e-10, x))\n",
    "    # WRITE ME\n",
    "\n",
    "def get_params(layers):\n",
    "    params_all = []\n",
    "    for layer in layers:\n",
    "        params = layer.params\n",
    "        params_all.extend(params)\n",
    "    return params_all\n",
    "    \n",
    "def compute_l1_reg(params):\n",
    "    l1_reg = 0\n",
    "    for param in params:\n",
    "        l1_reg += tf.reduce_sum(tf.abs(param))\n",
    "    return l1_reg\n",
    "\n",
    "def compute_l2_reg(params):\n",
    "    l2_reg = 0\n",
    "    for param in params:\n",
    "        l2_reg += tf.reduce_sum(tf.square(param)) # 2 * tf.nn.l2_lossを使っても良い\n",
    "    return l2_reg\n",
    "\n",
    "x_train, x_test, t_train = load_mnist()\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.1, random_state=random_state)\n",
    "\n",
    "### ネットワーク ###\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "lmd = 0.0001\n",
    "dropout_keep_prob = 0.75\n",
    "\n",
    "\n",
    "\n",
    "# # miniVGG network\n",
    "# config = tf.Variable(True)\n",
    "\n",
    "# h = Conv(filter_shape = (5, 5, 1, 6), function = tf.nn.relu)(x, config) # [None, 28, 28, 1] -> [None, 24, 24, 6]\n",
    "# h = Conv(filter_shape = (3, 3, 6, 6), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 24, 24, 6] -> [None, 24, 24, 6]\n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 24, 24, 6] -> [None, 12, 12, 6]\n",
    "\n",
    "# h = Conv(filter_shape = (5, 5, 6, 16), function = tf.nn.relu)(h, config) # [None, 12, 12, 6] -> [None, 8, 8, 16]\n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 8, 8, 16] -> [None, 4, 4, 16]\n",
    "\n",
    "# print(\"Before Flatten, the shape of h is:{}\".format(h.shape))\n",
    "# h = Flatten()(h)\n",
    "# print(\"After Flatten, the shape of h is:{}\".format(h.shape))\n",
    "\n",
    "# h = Dense(256, 120, tf.nn.relu)(h)\n",
    "# h = Dense(120, 84, tf.nn.relu)(h)\n",
    "# y = Dense(84, 10, tf.nn.softmax)(h)\n",
    "\n",
    "\n",
    "# miniVGG network\n",
    "config = tf.Variable(True)\n",
    "\n",
    "# Block1\n",
    "h = Conv(filter_shape = (3, 3, 1, 64), function = tf.nn.relu, padding = 'SAME')(x, config) # [None, 28, 28, 1] -> [None, 28, 28, 64]\n",
    "h = Conv(filter_shape = (3, 3, 64, 64), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 28, 28, 64] -> [None, 28, 28, 64]\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 28, 28, 64] -> [None, 14, 14, 64]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "# Block2\n",
    "h = Conv(filter_shape = (3, 3, 64, 128), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 14, 14, 64] -> [None, 14, 14, 128]\n",
    "h = Conv(filter_shape = (3, 3, 128, 128), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 14, 14, 128] -> [None, 14, 14, 128]\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 14, 14, 128] -> [None, 7, 7, 128]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "# Block3\n",
    "h = Conv(filter_shape = (3, 3, 128, 256), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 7, 7, 128] -> [None, 7, 7, 256]\n",
    "h = Conv(filter_shape = (3, 3, 256, 256), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 7, 7, 256] -> [None, 7, 7, 256]\n",
    "h = Conv(filter_shape = (3, 3, 256, 256), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 7, 7, 256] -> [None, 7, 7, 256]\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1], padding = 'SAME')(h) # [None, 7, 7, 256] -> [None, 4, 4, 256]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "# Block4\n",
    "h = Conv(filter_shape = (3, 3, 256, 512), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 4, 4, 256] -> [None, 4, 4, 512]\n",
    "h = Conv(filter_shape = (3, 3, 512, 512), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 4, 4, 512] -> [None, 4, 4, 512]\n",
    "h = Conv(filter_shape = (3, 3, 512, 512), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 4, 4, 512] -> [None, 4, 4, 512]\n",
    "h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1], padding = 'SAME')(h) # [None, 4, 4, 512] -> [None, 2, 2, 512]\n",
    "h = Dropout(dropout_keep_prob)(h) # 1 of 4 inputs is randomly excluded\n",
    "\n",
    "print(\"Before Flatten, the shape of h is:{}\".format(h.shape))\n",
    "h = Flatten()(h)\n",
    "print(\"After Flatten, the shape of h is:{}\".format(h.shape))\n",
    "\n",
    "h = Dense(2048, 200, tf.nn.relu)(h)\n",
    "# h = Dense(200, 200, tf.nn.relu)(h)\n",
    "h = Dropout(0.5)(h)\n",
    "# h = Dense(512, 84, tf.nn.relu)(h)\n",
    "# h = Dropout(0.5)(h)\n",
    "y = Dense(200, 10, tf.nn.softmax)(h)\n",
    "\n",
    "\n",
    "cost = - tf.reduce_mean(tf.reduce_sum(t * tf_log(y), axis=1))\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "# optimizer = tf.train.AdadeltaOptimizer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "    \n",
    "# 単純に移動平均・移動分散を計算するだけではパラメータが更新されないので、以下のように書き換える必要あり。\n",
    "# crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.cnn.y, logits=self.cnn.logits)\n",
    "# loss_op = tf.reduce_mean(crossent)\n",
    "# optimizer = tf.train.AdamOptimizer(config['learning_rate'])\n",
    "# extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  # <- ここ\n",
    "# with tf.control_dependencies(extra_update_ops):  # <- ここ\n",
    "#     train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "### 学習 ###\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "n_batches = x_train.shape[0]//batch_size #Floor division(打ち切り除算)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        x_train_fmnist, t_train_fmnist = shuffle(x_train, t_train, random_state = random_state)\n",
    "        for batch in range(n_batches):\n",
    "            start = batch * batch_size\n",
    "            finish = start + batch_size\n",
    "            sess.run(train, feed_dict = {x: x_train_fmnist[start:finish], t:t_train_fmnist[start:finish], is_training:True})\n",
    "        y_pred_, valid_cost_ = sess.run([y, cost], \n",
    "                            feed_dict = {x: x_valid, t:t_valid, is_training:False}\n",
    "                            )\n",
    "#         if ((epoch+1) % 5 == 0):\n",
    "        print(\"EPOCH: {}, Validation_Cost: {:.3f}, Accuracy_Score: {:.3f}\".format(\n",
    "            epoch+1,\n",
    "            valid_cost_,\n",
    "            accuracy_score(t_valid.argmax(axis = 1), y_pred_.argmax(axis = 1))\n",
    "        ))\n",
    "    y_pred = sess.run(y, feed_dict = {x: x_test, is_training: False})\n",
    "    submission = pd.Series(y_pred.argmax(axis = 1), name='label')\n",
    "    submission.to_csv('/root/userspace/chap06/submission/submission_pred_VGG16_bn_adam.csv', header=True, index_label='id')\n",
    "\n",
    "# tb.show_graph(tf.get_default_graph().as_graph_def())\n",
    "\n",
    "# WRITE ME\n",
    "# submission = pd.Series(y_pred, name='label')\n",
    "# submission.to_csv('/root/userspace/chap06/materials/submission_pred.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Batch Normalization sample\n",
    "# config = tf.Variable(True)\n",
    "\n",
    "# h = Conv(filter_shape = (5, 5, 1, 6), function = tf.nn.relu)(x, config) # [None, 28, 28, 1] -> [None, 24, 24, 6]\n",
    "# h = Conv(filter_shape = (3, 3, 6, 6), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 24, 24, 6] -> [None, 24, 24, 6]\n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 24, 24, 6] -> [None, 12, 12, 6]\n",
    "\n",
    "# h = Conv(filter_shape = (5, 5, 6, 16), function = tf.nn.relu)(h, config) # [None, 12, 12, 6] -> [None, 8, 8, 16]\n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 8, 8, 16] -> [None, 4, 4, 16]\n",
    "\n",
    "# print(\"Before Flatten, the shape of h is:{}\".format(h.shape))\n",
    "# h = Flatten()(h)\n",
    "# print(\"After Flatten, the shape of h is:{}\".format(h.shape))\n",
    "\n",
    "\n",
    "# # miniVGG network\n",
    "# config = tf.Variable(True)\n",
    "\n",
    "# h = Conv(filter_shape = (3, 3, 1, 24), function = tf.nn.relu, padding = 'SAME')(x, config) # [None, 28, 28, 1] -> [None, 28, 28, 24]\n",
    "# h = Conv(filter_shape = (3, 3, 24, 24), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 28, 28, 24] -> [None, 28, 28, 24]\n",
    "# h = Conv(filter_shape = (3, 3, 24, 24), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 28, 28, 24] -> [None, 28, 28, 24]\n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 28, 28, 24] -> [None, 14, 14, 24]\n",
    "\n",
    "# h = Conv(filter_shape = (3, 3, 24, 48), function = tf.nn.relu, padding = 'SAME')(h, config) \n",
    "# h = Conv(filter_shape = (3, 3, 48, 48), function = tf.nn.relu, padding = 'SAME')(h, config) \n",
    "# h = Conv(filter_shape = (3, 3, 48, 48), function = tf.nn.relu, padding = 'SAME')(h, config) \n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1])(h) # [None, 14, 14, 48] -> [None, 7, 7, 48]\n",
    "\n",
    "# h = Conv(filter_shape = (3, 3, 48, 48), function = tf.nn.relu, padding = 'SAME')(h, config) # [None, 7, 7, 128] -> [None, 7, 7, 128]\n",
    "# h = Conv(filter_shape = (3, 3, 48, 48), function = tf.nn.relu, padding = 'SAME')(h, config) \n",
    "# h = Conv(filter_shape = (3, 3, 48, 48), function = tf.nn.relu, padding = 'SAME')(h, config) \n",
    "# h = Pooling(ksize = (1, 2, 2, 1), strides = [1, 2, 2, 1], padding = 'SAME')(h) # [None, 7, 7, 48] -> [None, 4, 4, 48]\n",
    "\n",
    "# print(\"Before Flatten, the shape of h is:{}\".format(h.shape))\n",
    "# h = Flatten()(h)\n",
    "# print(\"After Flatten, the shape of h is:{}\".format(h.shape))\n",
    "\n",
    "# h = Dense(768, 200, tf.nn.relu)(h)\n",
    "# h = Dropout(dropout_keep_prob)(h)\n",
    "# h = Dense(200, 84, tf.nn.relu)(h)\n",
    "# h = Dropout(dropout_keep_prob)(h)\n",
    "# y = Dense(84, 10, tf.nn.softmax)(h)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
